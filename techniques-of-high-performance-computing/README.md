This repository contains four assignments completed as part of the Techniques of High-Performance Computing module. The work focuses on numerical methods, sparse matrix techniques, performance optimisation, and GPU acceleration using Python-based scientific computing tools.

The first assignment investigates the computational performance of matrix–matrix multiplication. A naïve triple-loop implementation is improved to achieve better efficiency, and its runtime is compared with NumPy’s optimised routines. The assignment explores the impact of algorithmic structure, JIT compilation using Numba, and memory layout choices on computational speed, supported by timing experiments and scalability analysis.

The second assignment focuses on solving differential equations using finite difference methods. A time-harmonic wave problem is discretised and written as a sparse matrix–vector system, which is solved using appropriate sparse solvers. The numerical error and convergence behaviour are analysed as the discretisation is refined. The assignment also includes the numerical solution of the heat equation using both explicit and implicit time-stepping schemes, together with an investigation of stability properties and computational performance.

The third assignment explores the internal structure and implementation of sparse matrices. A custom Compressed Sparse Row (CSR) matrix format is implemented, including matrix addition and matrix–vector multiplication. The performance of the custom implementation is compared against standard numerical routines, and a structured matrix is implemented using a LinearOperator approach to study computational efficiency and representation strategies.

The fourth assignment studies the numerical solution of the two-dimensional heat equation on a square domain. Finite difference discretisation is applied in space, and both explicit and implicit time-stepping methods are implemented. The convergence of the numerical approximation towards a known reference solution is analysed, and stability behaviour is examined. GPU acceleration is used to improve the performance of the explicit time-stepping scheme, highlighting the benefits of parallel computation in large-scale numerical simulations.

Across these assignments, key high-performance computing concepts are developed, including sparse linear algebra, numerical stability, convergence analysis, memory efficiency, performance benchmarking, and GPU-based acceleration using Numba CUDA.
