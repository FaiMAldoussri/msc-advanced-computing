{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 3."
      ],
      "metadata": {
        "id": "BN-scfWnlXXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzOykUxRUweQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099d38d0-4915-4175-bf68-d190fe1c37db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Degree    Train Error                   Test Error                    \n",
            "1         6.84% ± 0.14%          7.55% ± 0.57%\n",
            "2         4.54% ± 0.10%          6.02% ± 0.41%\n",
            "3         3.72% ± 0.09%          5.56% ± 0.64%\n",
            "4         3.80% ± 0.11%          5.90% ± 0.63%\n",
            "5         3.84% ± 0.08%          6.44% ± 0.50%\n",
            "6         3.83% ± 0.09%          6.83% ± 0.60%\n",
            "7         3.86% ± 0.10%          7.41% ± 0.61%\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "After careful testing and implementation, I determined that the optimal approach is to define both a convergence rate\n",
        "and a maximum number of epochs. This ensures that we avoid situations where the classifier fails to converge, which could\n",
        "lead to infinite loops. To identify the appropriate parameters, I conducted experiments using mini training and testing\n",
        "datasets to observe how many epochs were typically required for each degree to converge. Based on these observations, I set\n",
        "the maximum number of epochs to 50, which provides a reasonable balance between efficiency and reliability.\n",
        "This combination allows the algorithm to terminate either when convergence is achieved or when the maximum number of\n",
        "epochs is reached, ensuring both robustness and computational efficiency\n",
        "'''\n",
        "\n",
        "\n",
        "# polynomial kernel Kd(p, q) = (p · q)^d\n",
        "def polynomial_kernel(X1, X2, d):\n",
        "    return jnp.power(jnp.dot(X1, X2.T), d)\n",
        "\n",
        "def train_test_split(X, y, degree):\n",
        "    # shuffle the data for and take 80% for training and 20% for testing\n",
        "    # split the data into training and test sets\n",
        "\n",
        "    ratio =0.8\n",
        "    shuffle = np.arange(len(X))\n",
        "    np.random.shuffle(shuffle)\n",
        "\n",
        "    shuffled_data_X = X[shuffle]\n",
        "    shuffled_data_y = y[shuffle]\n",
        "\n",
        "    split = int(len(X) * ratio)\n",
        "\n",
        "    X_train, X_test = shuffled_data_X[:split], shuffled_data_X[split:]\n",
        "    y_train, y_test = shuffled_data_y[:split], shuffled_data_y[split:]\n",
        "\n",
        "    # convert them into jax format for faster implementaion\n",
        "    y_train = jnp.array(y_train)\n",
        "    y_test = jnp.array(y_test)\n",
        "\n",
        "    # Bc it's time consuming to compute the kernals for each epoch, I've decided to precompute them and use them directly for each run\n",
        "    kernel_train = polynomial_kernel(X_train, X_train, degree)\n",
        "    kernel_test = polynomial_kernel(X_train, X_test, degree)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, kernel_train, kernel_test\n",
        "\n",
        "# this function will train the data using One-vs-Rest method\n",
        "# a step by step explanation for OvR can be found on answer 2\n",
        "def kernel_perceptron_train(y_train, kernel_train, classes):\n",
        "    max_epochs = 50 # max number of epochs\n",
        "    n_samples = kernel_train.shape[0]\n",
        "    n_classes = len(classes)\n",
        "    # the wegiht is set to zero for each class\n",
        "    alpha = jnp.zeros((n_classes, n_samples), dtype=jnp.float32)\n",
        "    convergance_rate = 0.01\n",
        "\n",
        "    # this helper function aims to update the weight\n",
        "    def update_weights(state):\n",
        "        alpha, epoch,prev_updates = state\n",
        "        # for each sample, create binary labels for all classes\n",
        "        y_binary = jnp.where(y_train[:, None] == classes, 1, -1)\n",
        "        # compute the predictions for all samples and classes\n",
        "        predictions = jnp.sign(jnp.dot(alpha, kernel_train.T))\n",
        "        # identify where the predictions are incorrect and calculate the updates accordingly\n",
        "        # transposes y_binary to match the shape of predictions\n",
        "        updates = jnp.where(predictions != y_binary.T, y_binary.T, 0)\n",
        "        alpha = alpha + updates\n",
        "        avg_updates = jnp.mean(jnp.abs(updates)) # calculate the avg number of updates\n",
        "        return alpha, epoch + 1, avg_updates\n",
        "\n",
        "     # a helper function that returns true if epoch is less than max_epochs\n",
        "    def check_epoch(state):\n",
        "        _, epoch, avg_updates = state\n",
        "        return (epoch < max_epochs) & (avg_updates > convergance_rate)\n",
        "\n",
        "    # settings at the start : zero indicates to epoch=0, float('inf') as there's no previous update info\n",
        "    alpha, _ ,_= jax.lax.while_loop(check_epoch, update_weights, (alpha, 0, float('inf')))\n",
        "\n",
        "    return alpha, classes\n",
        "\n",
        "# prediction of (OvR) method\n",
        "@jax.jit\n",
        "def kernel_perceptron_predict(alpha, classes, kernel_matrix):\n",
        "    # calculate the weighted_sums for each class\n",
        "    weighted_sums = jnp.dot(alpha, kernel_matrix)\n",
        "    # take the test sample that has highest confidence score\n",
        "    predicted_indices = jnp.argmax(weighted_sums, axis=0)\n",
        "    return classes[predicted_indices]\n",
        "\n",
        "# compute the error rate\n",
        "def compute_error(y_true, y_pred):\n",
        "    n=len(y_true)\n",
        "    errors = jnp.sum(y_true != y_pred)\n",
        "    return (errors /n ) * 100\n",
        "\n",
        "def main():\n",
        "  # read the data from zipcombo file and then convert the feature&labels\n",
        "  # the use of float32 here is to accelerate the execution time\n",
        "  data = []\n",
        "  with open('zipcombo.dat.txt', 'r') as f:\n",
        "      for line in f:\n",
        "              values = list(map(float, line.strip().split()))\n",
        "              data.append(values)\n",
        "      data = np.array(data, dtype=np.float32)\n",
        "  X = data[:, 1:].astype(jnp.float32)\n",
        "  y = data[:, 0].astype(jnp.float32)\n",
        "\n",
        "\n",
        "  poly_degrees = range(1, 8)\n",
        "  train_errors = []\n",
        "  test_errors = []\n",
        "\n",
        "  for d in poly_degrees:\n",
        "      compute_train_errors = []\n",
        "      compute_run_test_errors = []\n",
        "      for _ in range(20):\n",
        "          # first, split the train and test data\n",
        "          X_train, X_test, y_train, y_test, kernel_train, kernel_test = train_test_split(X, y, degree=d)\n",
        "\n",
        "          # train the data\n",
        "          classes = np.unique(y_train).astype(np.int32)\n",
        "          alpha, classes = kernel_perceptron_train(y_train, kernel_train, classes)\n",
        "\n",
        "          # calculate the rates of the errors (training data)\n",
        "          y_train_pred = kernel_perceptron_predict(alpha, classes, kernel_train)\n",
        "          train_error = compute_error(y_train, y_train_pred)\n",
        "\n",
        "          # calculate the rates of the errors (testing data)\n",
        "          y_test_pred = kernel_perceptron_predict(alpha, classes, kernel_test)\n",
        "          test_error = compute_error(y_test, y_test_pred)\n",
        "\n",
        "          compute_train_errors.append(train_error)\n",
        "          compute_run_test_errors.append(test_error)\n",
        "\n",
        "      train_errors.append(compute_train_errors)\n",
        "      test_errors.append(compute_run_test_errors)\n",
        "\n",
        "  #   for each degree, by averaging over the 20 runs, calculate mean and standard deviation for train and test errors\n",
        "  avg_train_errors = [(np.mean(errors), np.std(errors)) for errors in train_errors]\n",
        "  avg_test_errors = [(np.mean(errors), np.std(errors)) for errors in test_errors]\n",
        "\n",
        "\n",
        "  print(f\"{'Degree ':<10}{'Train Error':<30}{'Test Error':<30}\")\n",
        "  for d, (train, test) in zip(poly_degrees, zip(avg_train_errors, avg_test_errors)):\n",
        "      train_mean, train_std = train\n",
        "      test_mean, test_std = test\n",
        "      print(f\"{d:<10}{train_mean:.2f}% ± {train_std:.2f}%{'':<10}{test_mean:.2f}% ± {test_std:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that as the polynomial degree increases from 1 to 3, the train and test errors decrease significantly. This suggests that lower degrees are not capturing the complexity of the patterns in the dataset effectively. However, as the degree increases further (4 to 7), the errors begin to plateau, indicating that the model is reaching its capacity to generalize the data effectively. The lowest test error is observed at degree 3, and interestingly, beyond this degree, the test error slightly increases, which might indicate slight overfitting. As expected, the train errors are consistently lower than the test errors, confirming that the model fits the training data better than it does the unseen test data. Additionally, the standard deviation percentages for both train and test errors remain relatively low, indicating stability in the performance across multiple runs."
      ],
      "metadata": {
        "id": "F8gvIKnpjJ1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 4 and 5."
      ],
      "metadata": {
        "id": "ChiFEAh8wfvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# polynomial kernel Kd(p, q) = (p · q)^d\n",
        "@jax.jit\n",
        "def polynomial_kernel(X1, X2, d):\n",
        "    return jnp.power(jnp.dot(X1, X2.T), d)\n",
        "\n",
        "def train_test_split(X, y):\n",
        "    # shuffle the data for and take 80% for training and 20% for testing\n",
        "    # split the data into training and test sets\n",
        "\n",
        "    ratio =0.8\n",
        "    shuffle = np.arange(len(X))\n",
        "    np.random.shuffle(shuffle)\n",
        "\n",
        "    shuffled_data_X = X[shuffle]\n",
        "    shuffled_data_y = y[shuffle]\n",
        "\n",
        "    split = int(len(X) * ratio)\n",
        "\n",
        "    X_train, X_test = shuffled_data_X[:split], shuffled_data_X[split:]\n",
        "    y_train, y_test = shuffled_data_y[:split], shuffled_data_y[split:]\n",
        "\n",
        "    # convert them into jax format for faster implementaion\n",
        "    y_train = jnp.array(y_train)\n",
        "    y_test = jnp.array(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# this function will train the data using One-vs-Rest method\n",
        "# a step by step explanation for OvR can be found on answer 2\n",
        "def kernel_perceptron_train(y_train, kernel_train, classes):\n",
        "    max_epochs = 50 # max number of epochs\n",
        "    n_samples = kernel_train.shape[0]\n",
        "    n_classes = len(classes)\n",
        "    # the wegiht is set to zero for each class\n",
        "    alpha = jnp.zeros((n_classes, n_samples), dtype=jnp.float32)\n",
        "    convergance_rate = 0.01\n",
        "\n",
        "    # this helper function aims to update the weight\n",
        "    def update_weights(state):\n",
        "        alpha, epoch,prev_updates = state\n",
        "        # for each sample, create binary labels for all classes\n",
        "        y_binary = jnp.where(y_train[:, None] == classes, 1, -1)\n",
        "        # compute the predictions for all samples and classes\n",
        "        predictions = jnp.sign(jnp.dot(alpha, kernel_train.T))\n",
        "        # identify where the predictions are incorrect and calculate the updates accordingly\n",
        "        # transposes y_binary to match the shape of predictions\n",
        "        updates = jnp.where(predictions != y_binary.T, y_binary.T, 0)\n",
        "        alpha = alpha + updates\n",
        "        avg_updates = jnp.mean(jnp.abs(updates)) # calculate the avg number of updates\n",
        "        return alpha, epoch + 1, avg_updates\n",
        "\n",
        "     # a helper function that returns true if epoch is less than max_epochs\n",
        "    def check_epoch(state):\n",
        "        _, epoch, avg_updates = state\n",
        "        return (epoch < max_epochs) & (avg_updates > convergance_rate)\n",
        "\n",
        "    # settings at the start : zero indicates to epoch=0, float('inf') as there's no previous update info\n",
        "    alpha, _ ,_= jax.lax.while_loop(check_epoch, update_weights, (alpha, 0, float('inf')))\n",
        "\n",
        "    return alpha, classes\n",
        "\n",
        "# prediction of (OvR) method\n",
        "@jax.jit\n",
        "def kernel_perceptron_predict(alpha, classes, kernel_matrix):\n",
        "    # calculate the weighted_sums for each class\n",
        "    weighted_sums = jnp.dot(alpha, kernel_matrix)\n",
        "    # take the test sample that has highest confidence score\n",
        "    predicted_indices = jnp.argmax(weighted_sums, axis=0)\n",
        "    return classes[predicted_indices]\n",
        "\n",
        "# compute error\n",
        "def compute_error(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    errors = jnp.sum(y_true != y_pred)\n",
        "    return (errors / n) * 100\n",
        "\n",
        "# compute confusion rate\n",
        "def compute_confusion_matrix(y_true, y_pred, y):\n",
        "    classes = np.unique(y)  # get the classes\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred, labels=classes) # build the confusion matrix\n",
        "    #We set the diagonal matrix to zero to exclude the true predictions, therefore we'll only only count the false predictions\n",
        "    np.fill_diagonal(conf_matrix, 0)\n",
        "    rows= conf_matrix.sum(axis=1, keepdims=True)\n",
        "    rows[rows == 0] = 1 # to avoid the devision by zero\n",
        "    #  “Number of times digit a was mistaken for digit b (test set)”/“Number of digit a points (test set)\n",
        "    rate = conf_matrix / rows\n",
        "    return rate\n",
        "\n",
        "# precompute kernals to speed up the computaion of cross validation\n",
        "def precompute_kernels(X, degrees):\n",
        "    return {d: polynomial_kernel(X, X, d) for d in degrees}\n",
        "\n",
        "# cross validation function\n",
        "def cross_validate(X, y, degrees, precomputed_kernels):\n",
        "    folds=5\n",
        "    n_samples = len(X) # to get the number of samples in the dataset\n",
        "    fold_size = n_samples // folds # to obtain the size of each fold\n",
        "    arr_indices = np.arange(n_samples) # an array of ordered samples\n",
        "    np.random.shuffle(arr_indices)\n",
        "\n",
        "    best_degree_errors = [] # list to store the the validation errors for each degree\n",
        "\n",
        "    for d in degrees:\n",
        "        fold_errors = [] # to store the errors for each fold\n",
        "        kernel = precomputed_kernels[d] # to obtain the precomputed kernals for the current degree\n",
        "\n",
        "        for fold in range(folds):\n",
        "            val_indices = arr_indices[fold * fold_size : (fold + 1) * fold_size] # select the indices for the validation set for the current fold\n",
        "            train_indices = np.setdiff1d(arr_indices, val_indices) # extract all the indices not in val_indices\n",
        "\n",
        "            # extract the training and validation kernals by slicing the precomputed kernal\n",
        "            kernel_train = kernel[train_indices][:, train_indices]\n",
        "            kernel_val = kernel[train_indices][:, val_indices]\n",
        "\n",
        "            y_train, y_val = y[train_indices], y[val_indices] # split the train and validation y\n",
        "            classes = np.unique(y_train).astype(np.int32)\n",
        "            alpha, _ = kernel_perceptron_train(y_train, kernel_train, classes) # train\n",
        "            y_val_pred = kernel_perceptron_predict(alpha, classes, kernel_val) # predict\n",
        "\n",
        "            val_error = compute_error(y_val, y_val_pred) # compute the error\n",
        "            fold_errors.append(val_error) # store the errors\n",
        "\n",
        "        mean_error = np.mean(fold_errors) # for the current polynomial degree, compute the mean of all folds\n",
        "        best_degree_errors.append((d, mean_error))\n",
        "\n",
        "    best_d = min(best_degree_errors, key=lambda x: x[1])[0] # retrieve the degree that has the lowest error\n",
        "    return best_d\n",
        "\n",
        "def main():\n",
        "  # read the data from zipcombo file and then convert the feature&labels\n",
        "  # the use of float32 here is to accelerate the execution time\n",
        "  data = []\n",
        "  with open('zipcombo.dat.txt', 'r') as f:\n",
        "      for line in f:\n",
        "              values = list(map(float, line.strip().split()))\n",
        "              data.append(values)\n",
        "      data = np.array(data, dtype=np.float32)\n",
        "  X = data[:, 1:].astype(jnp.float32)\n",
        "  y = data[:, 0].astype(jnp.float32)\n",
        "\n",
        "  poly_degrees = range(1, 8)\n",
        "  train_errors = []\n",
        "  test_errors = []\n",
        "  arr_best_degrees = []\n",
        "  confusion_matrices = []\n",
        "\n",
        "  # To speed up the excution time, we precompute full kernels for each degree\n",
        "  precomputed_kernels = precompute_kernels(X, poly_degrees)\n",
        "\n",
        "  for _ in range(20):\n",
        "\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "      # perform cross validation with precomputed kernels to obtain the best degree\n",
        "      best_degree = cross_validate(X_train, y_train, poly_degrees, precomputed_kernels)\n",
        "      arr_best_degrees.append(best_degree)\n",
        "\n",
        "      # now we retrain with the best degree\n",
        "      kernel_train = polynomial_kernel(X_train, X_train, best_degree)\n",
        "      kernel_test = polynomial_kernel(X_train, X_test, best_degree)\n",
        "      classes = np.unique(y_train).astype(np.int32)\n",
        "      alpha, classes = kernel_perceptron_train(y_train, kernel_train, classes)\n",
        "      # predict\n",
        "      y_train_pred = kernel_perceptron_predict(alpha, classes, kernel_train)\n",
        "      y_test_pred = kernel_perceptron_predict(alpha, classes, kernel_test)\n",
        "\n",
        "      # compute the errors of train and test\n",
        "      train_error = compute_error(y_train, y_train_pred)\n",
        "      test_error = compute_error(y_test, y_test_pred)\n",
        "\n",
        "      train_errors.append(train_error)\n",
        "      test_errors.append(test_error)\n",
        "      # compute the rate of the confusion matrix\n",
        "      conf_matrix = compute_confusion_matrix(y_test, y_test_pred, y)\n",
        "      confusion_matrices.append(conf_matrix)\n",
        "\n",
        "# prin and plot the results\n",
        "  print(\"\\nCross-Validation Results:\")\n",
        "  for run in range(20):\n",
        "      print(f\"Run {run + 1}: Best Degree = {arr_best_degrees[run]}, Train Error = {train_errors[run]:.2f}%, Test Error = {test_errors[run]:.2f}%\")\n",
        "\n",
        "  mean_best_degree = np.mean(arr_best_degrees)\n",
        "  std_best_degree = np.std(arr_best_degrees)\n",
        "  mean_train_error = np.mean(train_errors)\n",
        "  std_train_error = np.std(train_errors)\n",
        "  mean_test_error = np.mean(test_errors)\n",
        "  std_test_error = np.std(test_errors)\n",
        "\n",
        "  print(\"\\n Across Runs :\")\n",
        "  print(f\"Best Degree (mean ± std): {mean_best_degree:.2f} ± {std_best_degree:.2f}\")\n",
        "  print(f\"Train Error (mean ± std %): {mean_train_error:.2f}% ± {std_train_error:.2f}%\")\n",
        "  print(f\"Test Error (mean ± std %): {mean_test_error:.2f}% ± {std_test_error:.2f}%\")\n",
        "\n",
        "\n",
        "  mean_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
        "  std_confusion_matrix = np.std(confusion_matrices, axis=0)\n",
        "\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(mean_confusion_matrix, cmap='viridis', interpolation='nearest')\n",
        "  plt.colorbar()\n",
        "  plt.title(\"Test Set Error Confusion Matrix Mean\")\n",
        "  plt.xlabel(\"Predicted Labels\")\n",
        "  plt.ylabel(\"Actual Labels\")\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(std_confusion_matrix, cmap='viridis', interpolation='nearest')\n",
        "  plt.colorbar()\n",
        "  plt.title(\"Test Set Error Confusion Matrix Std. Dev.\")\n",
        "  plt.xlabel(\"Predicted Labels\")\n",
        "  plt.ylabel(\"Actual Labels\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  num_classes = mean_confusion_matrix.shape[0]\n",
        "\n",
        "  header = \"Predicted Labels\".center(15) + \"\".join([f\"{i}\".center(12) for i in range(num_classes)])\n",
        "  print(header)\n",
        "  print(\"-\" * len(header))\n",
        "\n",
        "  for i in range(num_classes):\n",
        "    row = f\"Actual Label {i}\".ljust(15)\n",
        "    for j in range(num_classes):\n",
        "            mean_val = mean_confusion_matrix[i, j]\n",
        "            std_val = std_confusion_matrix[i, j]\n",
        "            cell = f\"{mean_val:.2f}%±{std_val:.2f}%\"\n",
        "            row += cell.center(12)\n",
        "    print(row)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "sGLwX-ad6zjC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4443eba3-6ddc-44ae-e6c5-8380845c3109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Validation Results:\n",
            "Run 1: Best Degree = 3, Train Error = 3.79%, Test Error = 5.05%\n",
            "Run 2: Best Degree = 2, Train Error = 4.48%, Test Error = 6.56%\n",
            "Run 3: Best Degree = 4, Train Error = 3.78%, Test Error = 6.18%\n",
            "Run 4: Best Degree = 4, Train Error = 3.80%, Test Error = 6.18%\n",
            "Run 5: Best Degree = 3, Train Error = 3.90%, Test Error = 5.54%\n",
            "Run 6: Best Degree = 4, Train Error = 3.83%, Test Error = 7.15%\n",
            "Run 7: Best Degree = 4, Train Error = 4.05%, Test Error = 5.70%\n",
            "Run 8: Best Degree = 3, Train Error = 3.54%, Test Error = 6.77%\n",
            "Run 9: Best Degree = 5, Train Error = 3.89%, Test Error = 6.34%\n",
            "Run 10: Best Degree = 4, Train Error = 3.75%, Test Error = 6.72%\n",
            "Run 11: Best Degree = 2, Train Error = 4.46%, Test Error = 6.34%\n",
            "Run 12: Best Degree = 3, Train Error = 3.87%, Test Error = 5.97%\n",
            "Run 13: Best Degree = 4, Train Error = 3.82%, Test Error = 6.83%\n",
            "Run 14: Best Degree = 2, Train Error = 4.50%, Test Error = 6.02%\n",
            "Run 15: Best Degree = 4, Train Error = 3.83%, Test Error = 7.37%\n",
            "Run 16: Best Degree = 3, Train Error = 3.59%, Test Error = 6.34%\n",
            "Run 17: Best Degree = 4, Train Error = 3.86%, Test Error = 6.72%\n",
            "Run 18: Best Degree = 1, Train Error = 6.99%, Test Error = 6.61%\n",
            "Run 19: Best Degree = 2, Train Error = 4.49%, Test Error = 5.65%\n",
            "Run 20: Best Degree = 4, Train Error = 3.94%, Test Error = 5.81%\n",
            "\n",
            " Across Runs :\n",
            "Best Degree (mean ± std): 3.25 ± 0.99\n",
            "Train Error (mean ± std %): 4.11% ± 0.72%\n",
            "Test Error (mean ± std %): 6.29% ± 0.56%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Labels     0           1           2           3           4           5           6           7           8           9      \n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "Actual Label 0 0.00%±0.00% 0.00%±0.00% 0.21%±0.12% 0.07%±0.08% 0.15%±0.09% 0.13%±0.12% 0.28%±0.14% 0.05%±0.07% 0.10%±0.14% 0.01%±0.02% \n",
            "Actual Label 1 0.00%±0.00% 0.00%±0.00% 0.00%±0.00% 0.20%±0.28% 0.28%±0.33% 0.02%±0.07% 0.20%±0.27% 0.00%±0.00% 0.23%±0.29% 0.03%±0.11% \n",
            "Actual Label 2 0.16%±0.10% 0.09%±0.09% 0.00%±0.00% 0.14%±0.09% 0.27%±0.12% 0.03%±0.05% 0.05%±0.06% 0.12%±0.10% 0.12%±0.08% 0.03%±0.04% \n",
            "Actual Label 3 0.05%±0.05% 0.00%±0.02% 0.16%±0.07% 0.00%±0.00% 0.02%±0.03% 0.36%±0.11% 0.00%±0.00% 0.07%±0.06% 0.29%±0.13% 0.04%±0.05% \n",
            "Actual Label 4 0.06%±0.07% 0.26%±0.17% 0.23%±0.14% 0.00%±0.00% 0.00%±0.00% 0.01%±0.03% 0.15%±0.10% 0.03%±0.04% 0.03%±0.04% 0.22%±0.18% \n",
            "Actual Label 5 0.17%±0.08% 0.01%±0.02% 0.07%±0.05% 0.20%±0.12% 0.21%±0.10% 0.00%±0.00% 0.07%±0.05% 0.03%±0.05% 0.14%±0.07% 0.10%±0.09% \n",
            "Actual Label 6 0.20%±0.13% 0.12%±0.12% 0.26%±0.13% 0.00%±0.00% 0.21%±0.12% 0.06%±0.09% 0.00%±0.00% 0.00%±0.00% 0.15%±0.13% 0.00%±0.00% \n",
            "Actual Label 7 0.00%±0.00% 0.07%±0.08% 0.11%±0.09% 0.03%±0.05% 0.31%±0.11% 0.09%±0.09% 0.00%±0.00% 0.00%±0.00% 0.10%±0.09% 0.29%±0.14% \n",
            "Actual Label 8 0.14%±0.07% 0.09%±0.07% 0.15%±0.09% 0.26%±0.10% 0.11%±0.07% 0.16%±0.07% 0.00%±0.00% 0.06%±0.07% 0.00%±0.00% 0.04%±0.05% \n",
            "Actual Label 9 0.02%±0.04% 0.15%±0.12% 0.03%±0.05% 0.02%±0.03% 0.28%±0.13% 0.04%±0.06% 0.00%±0.00% 0.35%±0.13% 0.10%±0.08% 0.00%±0.00% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Validation Results**:The 5-fold cross-validation method appears to work effectively in selecting the best polynomial degree for the kernel perceptron. According to the results, we see that the optimal polynomial degree varies accross the runs but on average, the best degree is around 3.25 ± 0.99. This indicates that the degrees around this average captures the patterns in the dataset effectively. Degree 3, in particular, achieves the lowest test error in several runs, showing a good balance between underfitting and overfitting, which helps the model generalize well to unseen data.The mean train error of 4.11% ± 0.72% and mean test error of 6.29% ± 0.56% indicate relatively consistent performance across the runs with low variability.\n",
        "\n",
        "**Confusion matrix**: We notice that some digits have higher misclassification rates with specific other digits, and some digits have very low error rates overall, indicating that the model performs well in identifying them. For example, digit 0 has consistently low off-diagonal values, suggesting it is easily distinguishable from other digits. The standard deviation values in the matrix indicate that certain misclassifications are consistent across the 20 runs; for instance, actual 1 is always predicted as 4 with an error rate of 0.33%. Similarly, in the mean confusion matrix we notice that actual label 3 is misclassified with predicted label 5 with a rate of 0.36%. The misclassifications are generally concentrated near the diagonal, reflecting confusion between digits with similar shapes or features (3 and 5, 8 and 9). Which is a typical pattern in digit classification tasks, where similar-looking digits are harder to differentiate."
      ],
      "metadata": {
        "id": "3tq4U58Nf9Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "\n",
        "# Clear JAX caches\n",
        "jax.clear_caches()\n"
      ],
      "metadata": {
        "id": "9fUDOdpBd4t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 6"
      ],
      "metadata": {
        "id": "k9tF7idY9CB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# polynomial kernel Kd(p, q) = (p · q)^d\n",
        "@jax.jit\n",
        "def polynomial_kernel(X1, X2, d):\n",
        "    return jnp.power(jnp.dot(X1, X2.T), d)\n",
        "\n",
        "def train_test_split(X, y):\n",
        "    # shuffle the data for and take 80% for training and 20% for testing\n",
        "    # split the data into training and test sets\n",
        "\n",
        "    ratio =0.8\n",
        "    shuffle = np.arange(len(X))\n",
        "    np.random.shuffle(shuffle)\n",
        "\n",
        "    shuffled_data_X = X[shuffle]\n",
        "    shuffled_data_y = y[shuffle]\n",
        "\n",
        "    split = int(len(X) * ratio)\n",
        "\n",
        "    X_train, X_test = shuffled_data_X[:split], shuffled_data_X[split:]\n",
        "    y_train, y_test = shuffled_data_y[:split], shuffled_data_y[split:]\n",
        "\n",
        "    # convert them into jax format for faster implementaion\n",
        "    y_train = jnp.array(y_train)\n",
        "    y_test = jnp.array(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# this function will train the data using One-vs-Rest method\n",
        "# a step by step explanation for OvR can be found on answer 2\n",
        "def kernel_perceptron_train(y_train, kernel_train, classes):\n",
        "    max_epochs = 50 # max number of epochs\n",
        "    n_samples = kernel_train.shape[0]\n",
        "    n_classes = len(classes)\n",
        "    # the wegiht is set to zero for each class\n",
        "    alpha = jnp.zeros((n_classes, n_samples), dtype=jnp.float32)\n",
        "    convergance_rate = 0.01\n",
        "\n",
        "    # this helper function aims to update the weight\n",
        "    def update_weights(state):\n",
        "        alpha, epoch,prev_updates = state\n",
        "        # for each sample, create binary labels for all classes\n",
        "        y_binary = jnp.where(y_train[:, None] == classes, 1, -1)\n",
        "        # compute the predictions for all samples and classes\n",
        "        predictions = jnp.sign(jnp.dot(alpha, kernel_train.T))\n",
        "        # identify where the predictions are incorrect and calculate the updates accordingly\n",
        "        # transposes y_binary to match the shape of predictions\n",
        "        updates = jnp.where(predictions != y_binary.T, y_binary.T, 0)\n",
        "        alpha = alpha + updates\n",
        "        avg_updates = jnp.mean(jnp.abs(updates)) # calculate the avg number of updates\n",
        "        return alpha, epoch + 1, avg_updates\n",
        "\n",
        "     # a helper function that returns true if epoch is less than max_epochs\n",
        "    def check_epoch(state):\n",
        "        _, epoch, avg_updates = state\n",
        "        return (epoch < max_epochs) & (avg_updates > convergance_rate)\n",
        "\n",
        "    # settings at the start : zero indicates to epoch=0, float('inf') as there's no previous update info\n",
        "    alpha, _ ,_= jax.lax.while_loop(check_epoch, update_weights, (alpha, 0, float('inf')))\n",
        "\n",
        "    return alpha, classes\n",
        "\n",
        "# prediction of (OvR) method\n",
        "@jax.jit\n",
        "def kernel_perceptron_predict(alpha, classes, kernel_matrix):\n",
        "    # calculate the weighted_sums for each class\n",
        "    weighted_sums = jnp.dot(alpha, kernel_matrix)\n",
        "    # take the test sample that has highest confidence score\n",
        "    predicted_indices = jnp.argmax(weighted_sums, axis=0)\n",
        "    return classes[predicted_indices]\n",
        "\n",
        "# compute error\n",
        "def compute_error(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    errors = jnp.sum(y_true != y_pred)\n",
        "    return (errors / n) * 100\n",
        "\n",
        "# compute confusion rate\n",
        "def compute_confusion_matrix(y_true, y_pred, y):\n",
        "    classes = np.unique(y)  # get the classes\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred, labels=classes) # build the confusion matrix\n",
        "    #We set the diagonal matrix to zero to exclude the true predictions, therefore we'll only only count the false predictions\n",
        "    np.fill_diagonal(conf_matrix, 0)\n",
        "    rows= conf_matrix.sum(axis=1, keepdims=True)\n",
        "    rows[rows == 0] = 1 # to avoid the devision by zero\n",
        "    #  “Number of times digit a was mistaken for digit b (test set)”/“Number of digit a points (test set)\n",
        "    rate = conf_matrix / rows\n",
        "    return rate\n",
        "\n",
        "# precompute kernals to speed up the computaion of cross validation\n",
        "def precompute_kernels(X, degrees):\n",
        "    return {d: polynomial_kernel(X, X, d) for d in degrees}\n",
        "\n",
        "# cross validation function\n",
        "def cross_validate(X, y, degrees, precomputed_kernels):\n",
        "    folds=5\n",
        "    n_samples = len(X) # to get the number of samples in the dataset\n",
        "    fold_size = n_samples // folds # to obtain the size of each fold\n",
        "    arr_indices = np.arange(n_samples) # an array of ordered samples\n",
        "    np.random.shuffle(arr_indices)\n",
        "\n",
        "    best_degree_errors = [] # list to store the the validation errors for each degree\n",
        "\n",
        "    for d in degrees:\n",
        "        fold_errors = [] # to store the errors for each fold\n",
        "        kernel = precomputed_kernels[d] # to obtain the precomputed kernals for the current degree\n",
        "\n",
        "        for fold in range(folds):\n",
        "            val_indices = arr_indices[fold * fold_size : (fold + 1) * fold_size] # select the indices for the validation set for the current fold\n",
        "            train_indices = np.setdiff1d(arr_indices, val_indices) # extract all the indices not in val_indices\n",
        "\n",
        "            # extract the training and validation kernals by slicing the precomputed kernal\n",
        "            kernel_train = kernel[train_indices][:, train_indices]\n",
        "            kernel_val = kernel[train_indices][:, val_indices]\n",
        "\n",
        "            y_train, y_val = y[train_indices], y[val_indices] # split the train and validation y\n",
        "            classes = np.unique(y_train).astype(np.int32)\n",
        "            alpha, _ = kernel_perceptron_train(y_train, kernel_train, classes) # train\n",
        "            y_val_pred = kernel_perceptron_predict(alpha, classes, kernel_val) # predict\n",
        "\n",
        "            val_error = compute_error(y_val, y_val_pred) # compute the error\n",
        "            fold_errors.append(val_error) # store the errors\n",
        "\n",
        "        mean_error = np.mean(fold_errors) # for the current polynomial degree, compute the mean of all folds\n",
        "        best_degree_errors.append((d, mean_error))\n",
        "\n",
        "    best_d = min(best_degree_errors, key=lambda x: x[1])[0] # retrieve the degree that has the lowest error\n",
        "    return best_d\n",
        "\n",
        "\n",
        "def main():\n",
        "    # read the data from zipcombo file and then convert the feature&labels\n",
        "    # the use of float32 here is to accelerate the execution time\n",
        "    data = []\n",
        "    with open('zipcombo.dat.txt', 'r') as f:\n",
        "        for line in f:\n",
        "                values = list(map(float, line.strip().split()))\n",
        "                data.append(values)\n",
        "        data = np.array(data, dtype=np.float32)\n",
        "    X = data[:, 1:].astype(jnp.float32)\n",
        "    y = data[:, 0].astype(jnp.float32)\n",
        "\n",
        "    poly_degrees = range(1, 8)\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "    confusion_matrix_sum = np.zeros((10, 10))\n",
        "\n",
        "\n",
        "\n",
        "    # To speed up the excution time, we precompute full kernels for each degree\n",
        "    precomputed_kernels = precompute_kernels(X, poly_degrees)\n",
        "\n",
        "    for _ in range(20):\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "        # perform cross validation with precomputed kernels to obtain the best degree\n",
        "        best_degree = cross_validate(X_train, y_train, poly_degrees, precomputed_kernels)\n",
        "\n",
        "        # now we retrain with the best degree\n",
        "        kernel_train = polynomial_kernel(X_train, X_train, best_degree)\n",
        "        kernel_test = polynomial_kernel(X_train, X_test, best_degree)\n",
        "        classes = np.unique(y_train).astype(np.int32)\n",
        "        alpha, classes = kernel_perceptron_train(y_train, kernel_train, classes)\n",
        "        # predict\n",
        "        y_train_pred = kernel_perceptron_predict(alpha, classes, kernel_train)\n",
        "        y_test_pred = kernel_perceptron_predict(alpha, classes, kernel_test)\n",
        "\n",
        "        # compute the rate of the confusion matrix\n",
        "        conf_matrix = compute_confusion_matrix(y_test, y_test_pred, y)\n",
        "        np.fill_diagonal(conf_matrix, 0)\n",
        "        confusion_matrix_sum += conf_matrix\n",
        "\n",
        "    # convert the confusion matrix into a 1D vector, sort the indices in descending order, convert the sorted indices back to\n",
        "    # 2D indices for the matrix (true and predicted labels) combine the true and predicted label indices into pairs,\n",
        "    # and slice the first 5 pairs.\n",
        "    top_misclassified_pairs = np.dstack(np.unravel_index(np.argsort(-confusion_matrix_sum.ravel()), (10, 10)))[0][:5]\n",
        "\n",
        "    # plot the 5 samples\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i, (true_label, pred_label) in enumerate(top_misclassified_pairs):\n",
        "        # find the sample index to retrieve the misclassified image from the test dataset\n",
        "        sample = next(\n",
        "            idx # index of the sample\n",
        "            for idx, (tl, pl) in enumerate(zip(y_test, y_test_pred))\n",
        "            if int(tl) == true_label and int(pl) == pred_label\n",
        "        )\n",
        "        image = X_test[sample]\n",
        "        plt.subplot(1, 5, i + 1)\n",
        "        plt.imshow(image.reshape(16, 16), cmap=\"grey\")\n",
        "        plt.title(f\"True: {true_label} | Predicted: {pred_label}\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "wzXmL5-e4Ma9",
        "outputId": "dcff0fd8-8fba-4407-89c5-3158aca0f9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 5 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAADgCAYAAAD19b5rAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmfElEQVR4nO3da3RU1d3H8V/InRC5xgCCIYRLwYgISIsmkFqBgoBWKAYFCRQMVgVcS6JQVBDw0tKKUi7NqoKlYC0tKF6QiwsEwStSJVDKLYFYeSSCQRKBQLKfF6xMCQnkbMjOTPD7WYsXGX+zz3/G2bPnP+fMOUHGGCMAAAAAAOBELX8XAAAAAADA5YzGGwAAAAAAh2i8AQAAAABwiMYbAAAAAACHaLwBAAAAAHCIxhsAAAAAAIdovAEAAAAAcIjGGwAAAAAAh2i8AQAAAABwiMb7EqSkpCgtLc3fZXjSokWLMrWuX79eQUFBWr9+vd9qOte5NaLmYU5ULeZEzZeWlqaUlBR/l+FJSkpKmVpzcnIUFBSkhQsX+q2mc51bI2oe1omqxTpR87FOVK1AXicuqfEOCgry9C+Q3qDOVlBQoPHjx6tZs2YKDw9Xu3btNG/evCrfzsKFC8s8HxEREWrTpo0eeOABff3111W+PZfefvttTZkyxd9llDNlypQLvgY3bdpULXXU5DlR+oHifP9mzJhRZdtiTlSPPXv2aNCgQapfv75q166tpKQkrVu3rlprqMlzQpIeeughderUSQ0aNFDt2rXVrl07TZkyRQUFBVW6nXPnX2hoqFq2bKl77rlH+/btq9JtubZ582ZNmTJF+fn5/i6lnPO9/p555hm/11BT5sTZ9u7dq4iICAUFBenTTz+t0rFZJ6rHjBkzNGDAAMXGxiooKMgvddb0OfHqq69q6NChat26tYKCgpw1fawT1e/999/3Pd/ffPPNJY8Xcil3XrRoUZm///KXv2jNmjXlbm/Xrt2lbMaJ4uJi9e7dW59++qnuv/9+tW7dWqtWrdKvf/1rffvtt5o0aVKVb/PJJ59UfHy8Tpw4offff1/z5s3T22+/raysLNWuXbvKt3ch3bt31/HjxxUWFmZ1v7fffltz5swJuAXkjjvuUKtWrcrdPmnSJBUUFOiGG26oljpq8pxo165duTqlM49p9erV6tWrV5VvkznhTm5urrp166bg4GBNmDBBUVFRWrBggXr16qV3331X3bt3r5Y6avKckKRPPvlEycnJGjFihCIiIrR161Y988wzWrt2rTZs2KBatar2wLGxY8fqhhtu0KlTp/TZZ58pMzNTb731lrZt26amTZtW6bYqExcXp+PHjys0NNTqfps3b9bUqVOVlpamevXquSnuEvTs2VP33HNPmduuv/76att+TZ8TZ3vooYcUEhKikydPOtsG64RbkydPVuPGjXX99ddr1apVfqmhps+JefPmacuWLbrhhht0+PBh59tjnageJSUlevDBBxUVFaXCwsIqGfOSGu+hQ4eW+fvDDz/UmjVryt1+ru+//77a3yzPtWzZMm3evFkvvviiRo4cKUm67777NGjQIE2bNk2jRo3SlVdeWaXb7NOnj7p06SJJGjVqlBo2bKg//OEPev311zVkyJAK71NYWKioqKgqrUOSatWqpYiIiCof1186dOigDh06lLktNzdXX375pUaNGmW9SF6smjwnYmNjK6xz6tSpat26tZMvL5gT7jzzzDPKz89XVlaW2rZtK0kaPXq0fvSjH+mhhx7Sli1bqqWOmjwnpDPfdp8rISFBDz/8sD7++GP95Cc/qdLtJScna9CgQZKkESNGqE2bNho7dqxefvllTZw4scL7uJoTpXsZLzdt2rSp9PXnUk2fE6VWrVqlVatWKSMjQ9OnT3e2HdYJt7Kzs9WiRQt98803iomJ8UsNNX1OLFq0SFdddZVq1aqlxMRE59tjnagemZmZys3N1ahRo/T8889XyZjOf+OdkpKixMREbdmyRd27d1ft2rV9e5PPd0hLRb9Xyc/P1/jx49W8eXOFh4erVatWevbZZ1VSUlImd/DgQe3cuVOnTp26YF0bN26UJKWmppa5PTU1VSdOnNDrr79u+Ujt3XzzzZLOvOlJZ37jUadOHe3du1d9+/ZVdHS07r77bklnvnWZNWuWrrnmGkVERCg2Nlbp6en69ttvy4xpjNH06dPVrFkz1a5dWz/96U+1ffv2cts+3++UPvroI/Xt21f169dXVFSUOnTo4HuxpaWlac6cOZLKHhZUqqprlM4cxrZ3716vT2kZr7zyiowxvucwUATqnKjIxx9/rD179lTbc8icqLo5sXHjRl1//fW+pluSateurQEDBuizzz7T7t27Kx2jutSkOVG67dLtuXbunCj9Wc2OHTt01113qX79+kpKSvLl//rXv6pz586KjIxUgwYNlJqaqtzc3HLjZmZmKiEhQZGRkeratatvTTzb+X67t3PnTg0ePFgxMTGKjIxU27Zt9Zvf/MZX34QJEyRJ8fHxvjmRk5PjpEZJOnDggHbu3HmBZ7G848eP68SJE1b3qU6BPidOnTqlcePGady4cUpISLiox3ixWCeq9rNT6ftZoAvkOdG8efMqP/rJButE1a8TR44c0eTJk/Xkk09W6R75S9rj7dXhw4fVp08fpaamaujQoYqNjbW6//fff68ePXrov//9r9LT03X11Vdr8+bNmjhxog4ePKhZs2b5shMnTtTLL7/s+wbvfE6ePKng4OBye0JLvznbsmWLRo8ebVWnrdI3xYYNG/puO336tHr37q2kpCTNnDnTV096eroWLlyoESNGaOzYscrOztYf//hHbd26VZs2bfId4vH4449r+vTp6tu3r/r27avPPvtMvXr1UlFRUaX1rFmzRv369VOTJk00btw4NW7cWP/+97/15ptvaty4cUpPT9dXX31V4eE/rmr82c9+JkllJqNXixcvVvPmzavtkFobgTgnKrJ48WJJqrbGmzlRdXPi5MmTql+/frnbz36Pa926daXPQXUJ5Dlx+vRp5efnq6ioSFlZWZo8ebKio6PVtWtXy0dpr6I5IUm//OUv1bp1az311FMyxkg681vNxx57TIMHD9aoUaOUl5en2bNnq3v37tq6davvw8OLL76o9PR03XjjjRo/frz27dunAQMGqEGDBmrevPkF6/niiy+UnJys0NBQ3XvvvWrRooX27t2rN954QzNmzNAdd9yhXbt26ZVXXtFzzz2nRo0aSZJvT5qLGu+55x699957vuehMgsXLtTcuXNljFG7du00efJk3XXXXZ7uW50CeU7MmjVL3377rSZPnqxly5ZZPrJLwzrh9rNTIAvkOeFPrBNVv0489thjaty4sdLT0zVt2jRP9/HEVKH777/fnDtkjx49jCQzf/78cnlJ5oknnih3e1xcnBk+fLjv72nTppmoqCiza9euMrlHH33UBAcHmwMHDvhuGz58uJFksrOzL1jr73//eyPJbNy4sdyYkky/fv0ueP/Sx3Z2neezYMECI8msXbvW5OXlmdzcXPO3v/3NNGzY0ERGRpovv/yyTO2PPvpomftv3LjRSDKLFy8uc/s777xT5vZDhw6ZsLAwc+utt5qSkhJfbtKkSUZSmVrXrVtnJJl169YZY4w5ffq0iY+PN3Fxcebbb78ts52zx6ro/7GrGo0581qIi4srt73KZGVlGUkmIyPD+r5VqSbNiXOdPn3axMbGmq5du3q+D3PCbY3GeJ8T/fv3N/Xq1TPfffddmdu7detmJJmZM2dWOoYLNXFOfPDBB0aS71/btm19r5PKDB8+3PTo0aPSXOnr76WXXjJ5eXnmq6++Mm+99ZZp0aKFCQoKMp988okxxpgnnnjCSDJDhgwpc/+cnBwTHBxsZsyYUeb2bdu2mZCQEN/tRUVF5sorrzQdO3Y0J0+e9OUyMzONpDK1ZmdnG0lmwYIFvtu6d+9uoqOjzf79+8ts5+zX7u9+97sKn18XNRrzv9ePFzfeeKOZNWuWef311828efNMYmKikWTmzp3r6f4u1LQ5cfDgQRMdHW3+9Kc/GWP+935e+hqtDOuE2xqNubjPTnl5eed9bVW3mjYnznbNNdd4es8/G+uE2xqNsVsnPv/8cxMcHGxWrVpljPnf85mXl+fp/hdSLcdFhIeHa8SIERd9/6VLlyo5OVn169fXN9984/t3yy23qLi4WBs2bPBlFy5cKGNMpd9O3XXXXapbt65GjhypNWvWKCcnR5mZmZo7d66kM4ehVbVbbrlFMTExat68uVJTU1WnTh0tX75cV111VZncfffdV+bvpUuXqm7duurZs2eZx9+5c2fVqVPHd5bitWvXqqioSA8++GCZw5jGjx9faW1bt25Vdna2xo8fX+6QirPHOh9XNebk5Fz03m6p+vbU2grEOXGud999V19//bXT55A54W5O3HfffcrPz9edd96prVu3ateuXRo/frzvzMMu3uMuRSDPifbt22vNmjV67bXXlJGRoaioqCo/q3mpkSNHKiYmRk2bNtWtt96qwsJCvfzyy77fuJYaM2ZMmb+XLVumkpISDR48uMzjb9y4sVq3bu17vX366ac6dOiQxowZU+aIr7S0NNWtW/eCteXl5WnDhg0aOXKkrr766jL/zcuccFXj+vXrPe/F2LRpk8aNG6cBAwZozJgx2rJlixITEzVp0iTmhMc58cgjj6hly5YaNWrURddmg3Wi+j47BbpAnRPVjXXC7ToxduxY9enTx8lJhavlUPOrrrrqkk5utXv3bn3xxRfnPenDoUOHrMds3LixVqxYoWHDhvme2CuuuEKzZ8/W8OHDVadOnYuu93zmzJmjNm3aKCQkRLGxsWrbtm2534SEhISoWbNmZW7bvXu3jh49et6TvZU+/v3790tSucNHY2JiKjzk9Gylh6lc7EkhqqNGr4wxWrJkiRITE8udcC1QBOKcONfixYsVHBysO++885LHOh/mhLs50adPH82ePVuPPvqoOnXqJElq1aqVZsyYoYyMDCfvcZcikOfEFVdcoVtuuUWSdNttt2nJkiW67bbb9Nlnn+m666676HEr8vjjjys5OVnBwcFq1KiR2rVrp5CQ8kt1fHx8mb93794tY8x5fz5Qerjq+V5vpZeluZDSy9VcypxwXaOtsLAwPfDAA74m/OzfQfpbIM6JDz/8UIsWLdK7775bbb9pZZ2ons9ONUEgzgl/YJ1wt068+uqr2rx5s7Kysi56jAuplsY7MjLSKl9cXFzm75KSEvXs2VMZGRkV5tu0aXNRdXXv3l379u3Ttm3bVFhYqOuuu05fffXVJY15IV27di33bdS5wsPDyy0oJSUluvLKK317cc/lr7NQni2Qaty0aZP279+vp59+utq2aStQ50Sp48ePa/ny5brlllusf0Nlgznh1gMPPKARI0boiy++UFhYmDp27KgXX3xRkpv3uEsR6HPibHfccYeGDRumv/3tb1XeeF977bW+Jv9Czn2+SkpKFBQUpJUrVyo4OLhcPhC+aAnUGkt/C3jkyBG/bP98AnFOZGRkKDk5WfHx8b49qqXXtj148KAOHDhQbi/XpWKdQKlAnBP+wDrhzoQJE/TLX/5SYWFhvve40hOp5ubmqqio6JIu2VYtjff51K9fv9xZYYuKinTw4MEytyUkJKigoMDTi8xWcHCwOnbs6Pt77dq1kuRkWxcrISFBa9eu1U033XTBN524uDhJZ74tOvvbnry8vHJnx6xoG5KUlZV1wcd+vsNEqqNGrxYvXqygoKCAPFlOZQJhTkjSihUrdOzYsYA9VJ854V1UVJS6devm+3vt2rWKjIzUTTfddMljV4dAmRNnO3nypEpKSnT06FHn2/IqISFBxhjFx8df8MPj2a+30jPhSmfOUp2dnX3BLxJKX5+V7Qm40JxwXePFKN1DU1OaHH/OiQMHDmj//v3l9qRJ0oABA1S3bt1qOdu/F6wTPxyBuE4EItaJyuXm5mrJkiVasmRJuf/WqVMnXXfddfrXv/51UWNL1XA5sQtJSEgo83sK6cxp4c/9hmrw4MH64IMPtGrVqnJj5Ofn6/Tp076/L+UyMXl5eXr22WfVoUOHgJqUgwcPVnFxcYVn1Ss926505suC0NBQzZ49u8zvGM4+S+P5dOrUSfHx8Zo1a1a5N6+zxyq9BuC5GVc12l5O7NSpU1q6dKmSkpKq/Fv36hAoc2LJkiWqXbu2fvGLX1g+gurBnLi4S+xt3rxZy5Yt069+9atKf6cVKPw5J/Lz8yvM/PnPf5akSvfCVac77rhDwcHBmjp1arnfsRljdPjwYUlnao6JidH8+fPLnA154cKFlTZMMTEx6t69u1566SUdOHCg3DZKnW9OuKrR62Vi8vLyyt127NgxzZo1S40aNVLnzp0rHSMQ+HNOZGZmavny5WX+Pfjgg5KkmTNnnnfPrT+wTlzcOlETBcpnp0DHOlH5OnHu+9vy5ct9P7n8y1/+oueee67SMS7Er3u8R40apTFjxmjgwIHq2bOnPv/8c61atcp3WvlSEyZM0IoVK9SvXz+lpaWpc+fOKiws1LZt2/SPf/xDOTk5vvvYnP6/R48e6tatm1q1aqX/+7//U2ZmpgoKCvTmm2/69Xp85+rRo4fS09P19NNP61//+pd69eql0NBQ7d69W0uXLtXzzz+vQYMGKSYmRg8//LCefvpp9evXT3379tXWrVu1cuXKcs/puWrVqqV58+apf//+6tixo0aMGKEmTZpo586d2r59u+9NqvSDydixY9W7d28FBwcrNTXVWY22l8RYtWqVDh8+HLB7aivj7zkhnTnccuXKlRo4cGBAHHZUEeZE5XNi//79Gjx4sAYMGKDGjRtr+/btmj9/vjp06KCnnnrqIp51//DnnFi/fr3Gjh2rQYMGqXXr1ioqKtLGjRu1bNkydenSRUOHDnX50K0kJCRo+vTpmjhxonJycnT77bcrOjpa2dnZWr58ue699149/PDDCg0N1fTp05Wenq6bb75Zd955p7Kzs7VgwQJPv4t74YUXlJSUpE6dOunee+/1HXL81ltv+fYClM6J3/zmN0pNTVVoaKj69+/vrEavl4mZM2eOXnvtNfXv319XX321Dh486PtwuGjRokv67Wh18uecqOhkQ6Ufcnv06BFQX0axTnj77LRo0SLt379f33//vSRpw4YNmj59uiRp2LBhvj2Lgczfn502bNjga/zz8vJUWFjoew67d+8eMJe1ZZ2ofJ24/fbby91WWnOfPn0qfU+o1CWfF/0s5zv9/zXXXFNhvri42DzyyCOmUaNGpnbt2qZ3795mz5495U7/b4wxx44dMxMnTjStWrUyYWFhplGjRubGG280M2fONEVFRb6czen/H3roIdOyZUsTHh5uYmJizF133WX27t3r+fHaXhKjskttDB8+3ERFRZ33v2dmZprOnTubyMhIEx0dba699lqTkZFhvvrqK1+muLjYTJ061TRp0sRERkaalJQUk5WVVe45PfeSGKXef/9907NnTxMdHW2ioqJMhw4dzOzZs33//fTp0+bBBx80MTExJigoqNz/76qs0Rj7S2Kkpqaa0NBQc/jwYc/3cammzQljjJk/f76RZFasWGH1WEsfG3MiMObEkSNHzG233WYaN25swsLCTHx8vHnkkUfKXV6sutWkObFnzx5zzz33mJYtW5rIyEgTERFhrrnmGvPEE0+YgoICT4/X9jIxS5cuvWCussua/POf/zRJSUkmKirKREVFmR/96Efm/vvvN//5z3/K5ObOnWvi4+NNeHi46dKli9mwYYPp0aNHpZeJMebM5Rp/8YtfmHr16pmIiAjTtm1b89hjj5XJTJs2zVx11VWmVq1a5Z7rqqzRGO+XiVm9erXp2bOnady4sQkNDTX16tUzvXr1Mu+++26l93WpJs2Jiri+nBjrhF2Nxth9diqdPxX983rZxKpW0+ZE6ftyRf+8XJ6NdSJw1omKVOXlxIKM8XhudZSTkpKiFi1aaOHChf4uBQgIzAmgrLS0NOXk5Gj9+vX+LgUICKwTQFmsEz8cgXM8NQAAAAAAlyEabwAAAAAAHKLxBgAAAADAIX7jDQAAAACAQ+zxBgAAAADAIRpvAAAAAAAcovEGAAAAAMChEK/BoKAgl3UEhBYtWnjOTp482WrstLQ0z9n8/HzP2d/97ndWdZSUlHjO/uc///GcXbFihVUdgeJSTnHwQ5gTNsLDw63yo0eP9pzt3Lmzk6wkbd++3XN2yJAhVmPXRNU1J2JjY63GnjlzpufsFVdcYTV2rVrev4Pu16+f1dhHjx71nH3vvfesxq6JbNcKmzXLZi5L0scff+w5e7HzgnXi0mRkZHjObtq0yUkW5fHZqWa49957rfI/+clPPGdHjhxpW85lz8u8YI83AAAAAAAO0XgDAAAAAOAQjTcAAAAAAA7ReAMAAAAA4BCNNwAAAAAADtF4AwAAAADgEI03AAAAAAAO0XgDAAAAAOAQjTcAAAAAAA6F+LsAAMAPz4QJE6zyQ4cOdVSJW3Xr1vWcHTBggMNKAkMgPcbevXv7uwRUIjg42HP2pptu8pzdtGnTxZQD1ChxcXFW+UGDBnnOjhkzxmrsoqIiq/zl6rJuvH/84x9b5VevXu05e8UVV1iNvXPnTs/ZgoICz9lp06ZZ1REaGuo5+8Ybb3jOrlixwqoOXH6effZZq/yxY8c8Z998803PWdvXYkZGhlUeAAAAsMWh5gAAAAAAOETjDQAAAACAQzTeAAAAAAA4ROMNAAAAAIBDNN4AAAAAADhE4w0AAAAAgEM03gAAAAAAOETjDQAAAACAQzTeAAAAAAA4ROMNAAAAAIBDIf4uAADww7No0SKr/KFDhxxVYqdz585W+aZNmzqqxE5SUpK/Swg4ycnJ/i4BlYiOjnaSBX4ImjdvbpUPDg72nC0uLrYtB7rMG+/f/va3VvmIiAjP2dTUVKuxly9f7jnbrVs3z9m5c+da1dG+fXvP2YYNG1qNjctPo0aNPGcHDBhgNXbLli1ty/GkS5cuVvnQ0FAndQAAAAClONQcAAAAAACHaLwBAAAAAHCIxhsAAAAAAIdovAEAAAAAcIjGGwAAAAAAh2i8AQAAAABwiMYbAAAAAACHaLwBAAAAAHCIxhsAAAAAAIdC/F0AAOCH5/PPP3eat9G8eXPP2Z07d1qNXbduXc/ZlJQUz9k+ffpY1fFD8OGHH1rlX3jhBc/ZyZMn25aDKlCvXj3P2by8PHeFADVQTEyMVf7777/3nC0pKbEtB7rMG+8DBw5Y5bt06eI5O3r0aKux58+f7zm7f/9+z9n27dtb1WGDSYX+/ft7zq5evdphJd7dcMMNVvkdO3Y4qgQAAAA4g0PNAQAAAABwiMYbAAAAAACHaLwBAAAAAHCIxhsAAAAAAIdovAEAAAAAcIjGGwAAAAAAh2i8AQAAAABwiMYbAAAAAACHaLwBAAAAAHCIxhsAAAAAAIdC/F2AS+PHj7fKJyQkeM4mJydbjZ2VleU527BhQ6uxXfnyyy/9XQL8LCUlxXN27dq17gqxcO2111rlN2/e7KiSH56YmBjP2d///vdWY8fHx3vOJiUlWY2NsgoKCqzyu3fv9pxdtmyZ1dibNm3ynF23bp3V2Ah8Nu8phw4dclgJUPPExsZa5QsLCz1njTG25UDs8QYAAAAAwCkabwAAAAAAHKLxBgAAAADAIRpvAAAAAAAcovEGAAAAAMAhGm8AAAAAAByi8QYAAAAAwCEabwAAAAAAHKLxBgAAAADAIRpvAAAAAAAcCvF3AQCAy8PYsWM9Z4cNG+awkpqpoKDAc/avf/2r1dg2+V27dlmNnZeXZ5VHYLv99tut8hs2bPCcPXLkiNXYNnPi66+/thobuNxFR0db5UtKShxVglKXdeN9+PBhq3xKSorn7N133+1s7G3btnnOxsbGWtXx8MMPe87u2bPHamxcfk6dOuU5u3r1aoeVeJecnGyV/9Of/uSoEgAAAOAMDjUHAAAAAMAhGm8AAAAAAByi8QYAAAAAwCEabwAAAAAAHKLxBgAAAADAIRpvAAAAAAAcovEGAAAAAMAhGm8AAAAAAByi8QYAAAAAwCEabwAAAAAAHArxdwGBpKioyHN2wYIFVmPb5r164403nIwrSf/973+djY2aYdSoUf4uQZIUEuL9rSoxMdFq7M8//9y2HJzHq6++6jnbrl07q7FjY2Nty/Hsxhtv9JytVcvd99V16tTxnB0+fLjV2HFxcZ6zkyZNsho7Ly/PKo/A9vOf/9wqv2PHDs/ZI0eOWI1dUFDgOVuvXj2rsQFXgoKCnOVtPg81a9bMqg4+D7nHHm8AAAAAAByi8QYAAAAAwCEabwAAAAAAHKLxBgAAAADAIRpvAAAAAAAcovEGAAAAAMAhGm8AAAAAAByi8QYAAAAAwCEabwAAAAAAHKLxBgAAAADAoRB/FwAAuDxkZWV5zg4aNMhhJXYaNGjgOZuUlGQ19p133ukkGxkZaVVHnz59PGd79eplNfaECRM8Z5977jmrsVH9QkLsPho2bdrUc3bXrl1WYxcUFHjONmnSxGpsG2FhYU7GLSoqcjIu/CsuLs4qv2HDBs/ZzMxMz9mIiAirOk6dOmWVhz0a7xqua9euVvmSkhLP2X/+85+25QBOdOnSxXN227ZtDisBAAAA7HGoOQAAAAAADtF4AwAAAADgEI03AAAAAAAO0XgDAAAAAOAQjTcAAAAAAA7ReAMAAAAA4BCNNwAAAAAADtF4AwAAAADgEI03AAAAAAAOhfi7AAAA/OnIkSOesytWrLAa2yb/6KOPes4+/vjjVnUMGzbMczY8PNxq7D/84Q+esx06dLAa+7nnnvOc/eKLL6zGRsVq1bLbJ2PzegkJsfvY2axZM8/Zjh07es6+8847VnW0b9/ec3bs2LGes6+99ppVHagZcnJyrPIvvfSS56zL97nvvvvO2dg4g8Y7ANm8wTds2NBq7E8++cRzNi8vz2pswJWBAwd6zq5cudJhJQAAAIA9DjUHAAAAAMAhGm8AAAAAAByi8QYAAAAAwCEabwAAAAAAHKLxBgAAAADAIRpvAAAAAAAcovEGAAAAAMAhGm8AAAAAAByi8QYAAAAAwCEabwAAAAAAHArxdwEob8iQIZ6zwcHBVmOvWbPGthzA7xITEz1n586d67ASwJ3c3FzP2dGjR1uN/eqrr3rOulwn0tLSrPJXXnml5+ytt95qWQ0qsm7dOqv8/PnzPWeLi4utxk5ISPCcXbp0qefsk08+aVXH9u3bPWeNMVZjA1OmTPGcvemmm5zV8fXXXzsbG2ewxxsAAAAAAIdovAEAAAAAcIjGGwAAAAAAh2i8AQAAAABwiMYbAAAAAACHaLwBAAAAAHCIxhsAAAAAAIdovAEAAAAAcIjGGwAAAAAAh2i8AQAAAABwKMTfBQAAcLmKjo72nB06dKjn7MCBA63q+PGPf2yVDxRdu3b1nI2IiLAa+8SJE7bl/CAsWrTIKr9lyxbP2R07dliNPWfOHM/ZDz74wHM2KyvLqg4gUAQFBTkbu6SkxNnYOIPGu5qEhoZ6zg4fPtxZHXv37nU2NuBK27ZtPWe3b9/usBIAAADAHoeaAwAAAADgEI03AAAAAAAO0XgDAAAAAOAQjTcAAAAAAA7ReAMAAAAA4BCNNwAAAAAADtF4AwAAAADgEI03AAAAAAAO0XgDAAAAAOAQjTcAAAAAAA6F+LuAH4rHH3/cc7Z58+aesx988IFVHcuWLbPKA4EgNjbWc/bgwYMOK0FNEBYWZpUfOHCg5+y0adOsxm7UqJHnbN26dT1nCwoKrOqYM2eOk6wkdenSxXPWdg2yef5++tOfWo29cuVKqzwqtmPHDmdj27zOCwsLndUBBAqXn3GaNGnibGycwR5vAAAAAAAcovEGAAAAAMAhGm8AAAAAAByi8QYAAAAAwCEabwAAAAAAHKLxBgAAAADAIRpvAAAAAAAcovEGAAAAAMAhGm8AAAAAAByi8QYAAAAAwKEQfxdQUzVq1MgqP27cOCd1/P3vf7fKf/fdd07qAFwKCgrynD1+/LjDSlBVbP6fSlJ6errn7JNPPmk1dkxMjOesMcZq7LfeestzdtmyZU6yknT06FHP2YiICKuxExMTrfI2bJ7vvLw8Z3XAP/Lz8z1nQ0ND3RUCBIjc3FzP2WPHjlmN3b59e9tyYIk93gAAAAAAOETjDQAAAACAQzTeAAAAAAA4ROMNAAAAAIBDNN4AAAAAADhE4w0AAAAAgEM03gAAAAAAOETjDQAAAACAQzTeAAAAAAA4ROMNAAAAAIBDNN4AAAAAADgU4u8Caqq0tDSrfHR0tOesMcZzdvXq1VZ1AIEiODjYc/b48eMOK4E/NGjQwCo/b948R5XYsXl/lqQdO3Z4zsbExHjOpqenW9XRv39/z9mmTZtajd2yZUurvI2PPvrIc/bTTz91Vgf848CBA56zJ0+edFgJEBiKioo8Z3ft2mU1dmJioudsRESE1dgnTpywyl+u2OMNAAAAAIBDNN4AAAAAADhE4w0AAAAAgEM03gAAAAAAOETjDQAAAACAQzTeAAAAAAA4ROMNAAAAAIBDNN4AAAAAADhE4w0AAAAAgEM03gAAAAAAOBTi7wJqqlatWjkbe9++fZ6zO3bscFYH4FJxcbHnbEREhMNK4A9Hjx61yr/yyiues0OGDLEtx7Natey+r87IyHBUSc2UnZ1tlR8xYoSjSlAT7Nmzx3P2+uuvd1gJUPOcPn3aKh8eHu45GxMTYzV2bm6uVf5yxR5vAAAAAAAcovEGAAAAAMAhGm8AAAAAAByi8QYAAAAAwCEabwAAAAAAHKLxBgAAAADAIRpvAAAAAAAcovEGAAAAAMAhGm8AAAAAAByi8QYAAAAAwCEabwAAAAAAHArxdwE11YIFC6zyffv29Zz99a9/bVsOcFnbt2+f52xERITV2CdOnLAtB1Xg9OnTVvnhw4d7zq5Zs8Zq7JSUFM/ZHj16WI0dFxdnlQ8Eb7zxhlV+586dnrNTp061GruwsNAqj8uLzXv/zTff7LASoOZ5++23rfKdOnVyVAlKsccbAAAAAACHaLwBAAAAAHCIxhsAAAAAAIdovAEAAAAAcIjGGwAAAAAAh2i8AQAAAABwiMYbAAAAAACHaLwBAAAAAHCIxhsAAAAAAIdovAEAAAAAcCjIGGP8XQQAAAAAAJcr9ngDAAAAAOAQjTcAAAAAAA7ReAMAAAAA4BCNNwAAAAAADtF4AwAAAADgEI03AAAAAAAO0XgDAAAAAOAQjTcAAAAAAA7ReAMAAAAA4ND/Aw7wqE766QyZAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is not surprising that these digits are hard to predict given their ambiguous shapes and distortions. Most of them do not resemble their actual labels, and without the labels, it would be challenging to identify them with confidence. The visual similarities to other digits and the distortions or inconsistencies in their patterns further add to the confusion. These factors make it reasonable to expect the model to struggle with such cases"
      ],
      "metadata": {
        "id": "fBAp1xYpi9Ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 7."
      ],
      "metadata": {
        "id": "3pPMwg1cQj9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) After performing experiments on the mini training and testing sets, we observed that for values of **C>1** the training error consistently reached 0.00%, indicating that the model was perfectly fitting the training data. However, this led to a significant increase in testing error, which suggests that the model was overfitting. Conversely, for very low values of\n",
        "**C**, both the training and testing errors were excessively high, indicating that the model was underfitting. After analyzing the results, the optimal range of **C** values that provided a reasonable balance between training and testing errors was found to be between 0.00078 and 0.062. So the set of **S** values are:[ 0.00078, 0.0016, 0.0034, 0.007, 0.014, 0.03, 0.062 ]"
      ],
      "metadata": {
        "id": "WUN6TaJnxW5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b)"
      ],
      "metadata": {
        "id": "CV7sNLzUQnDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# gaussian kernel function: Kc(p, q) = e^(-c * ||p - q||^2)\n",
        "@jax.jit\n",
        "def gaussian_kernel(X1, X2, c):\n",
        "    squared_norm = jnp.sum((X1[:, None] - X2) ** 2, axis=2)\n",
        "    return jnp.exp(-c * squared_norm)\n",
        "\n",
        "def train_test_split(X, y, c):\n",
        "    # shuffle the data for and take 80% for training and 20% for testing\n",
        "    # split the data into training and test sets\n",
        "\n",
        "    ratio =0.8\n",
        "    shuffle = np.arange(len(X))\n",
        "    np.random.shuffle(shuffle)\n",
        "\n",
        "    shuffled_data_X = X[shuffle]\n",
        "    shuffled_data_y = y[shuffle]\n",
        "\n",
        "    split = int(len(X) * ratio)\n",
        "\n",
        "    X_train, X_test = shuffled_data_X[:split], shuffled_data_X[split:]\n",
        "    y_train, y_test = shuffled_data_y[:split], shuffled_data_y[split:]\n",
        "\n",
        "    # convert them into jax format for faster implementaion\n",
        "    y_train = jnp.array(y_train)\n",
        "    y_test = jnp.array(y_test)\n",
        "\n",
        "    # Bc it's time consuming to compute the kernals for each epoch, I've decided to precompute them and use them directly for each run\n",
        "    kernel_train = gaussian_kernel(X_train, X_train, c)\n",
        "    kernel_test = gaussian_kernel(X_train, X_test, c)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, kernel_train, kernel_test\n",
        "\n",
        "# this function will train the data using One-vs-Rest method\n",
        "# a step by step explanation for OvR can be found on answer 2\n",
        "def kernel_perceptron_train(y_train, kernel_train, classes):\n",
        "    max_epochs = 50 # max number of epochs\n",
        "    n_samples = kernel_train.shape[0]\n",
        "    n_classes = len(classes)\n",
        "    # the wegiht is set to zero for each class\n",
        "    alpha = jnp.zeros((n_classes, n_samples), dtype=jnp.float32)\n",
        "    convergance_rate = 0.01\n",
        "\n",
        "    # this helper function aims to update the weight\n",
        "    def update_weights(state):\n",
        "        alpha, epoch,prev_updates = state\n",
        "        # for each sample, create binary labels for all classes\n",
        "        y_binary = jnp.where(y_train[:, None] == classes, 1, -1)\n",
        "        # compute the predictions for all samples and classes\n",
        "        predictions = jnp.sign(jnp.dot(alpha, kernel_train.T))\n",
        "        # identify where the predictions are incorrect and calculate the updates accordingly\n",
        "        # transposes y_binary to match the shape of predictions\n",
        "        updates = jnp.where(predictions != y_binary.T, y_binary.T, 0)\n",
        "        alpha = alpha + updates\n",
        "        avg_updates = jnp.mean(jnp.abs(updates)) # calculate the avg number of updates\n",
        "        return alpha, epoch + 1, avg_updates\n",
        "\n",
        "     # a helper function that returns true if epoch is less than max_epochs\n",
        "    def check_epoch(state):\n",
        "        _, epoch, avg_updates = state\n",
        "        return (epoch < max_epochs) & (avg_updates > convergance_rate)\n",
        "\n",
        "    # settings at the start : zero indicates to epoch=0, float('inf') as there's no previous update info\n",
        "    alpha, _ ,_= jax.lax.while_loop(check_epoch, update_weights, (alpha, 0, float('inf')))\n",
        "\n",
        "    return alpha, classes\n",
        "\n",
        "# prediction of (OvR) method\n",
        "@jax.jit\n",
        "def kernel_perceptron_predict(alpha, classes, kernel_matrix):\n",
        "    # calculate the weighted_sums for each class\n",
        "    weighted_sums = jnp.dot(alpha, kernel_matrix)\n",
        "    # take the test sample that has highest confidence score\n",
        "    predicted_indices = jnp.argmax(weighted_sums, axis=0)\n",
        "    return classes[predicted_indices]\n",
        "\n",
        "# compute the error rate\n",
        "def compute_error(y_true, y_pred):\n",
        "    n=len(y_true)\n",
        "    errors = jnp.sum(y_true != y_pred)\n",
        "    return (errors /n ) * 100\n",
        "\n",
        "def main():\n",
        "  # read the data from zipcombo file and then convert the feature&labels\n",
        "  # the use of float32 here is to accelerate the execution time\n",
        "  data = []\n",
        "  with open('zipcombo.dat.txt', 'r') as f:\n",
        "      for line in f:\n",
        "              values = list(map(float, line.strip().split()))\n",
        "              data.append(values)\n",
        "      data = np.array(data, dtype=np.float32)\n",
        "  X = data[:, 1:].astype(jnp.float32)\n",
        "  y = data[:, 0].astype(jnp.float32)\n",
        "\n",
        "\n",
        "  S = [7.8e-4, 1.6e-3, 3.4e-3, 7.0e-3, 1.4e-2, 3.0e-2, 6.2e-2]\n",
        "  train_errors = []\n",
        "  test_errors = []\n",
        "\n",
        "  for c in S:\n",
        "      compute_train_errors = []\n",
        "      compute_run_test_errors = []\n",
        "      for _ in range(20):\n",
        "          # first, split the train and test data\n",
        "          X_train, X_test, y_train, y_test, kernel_train, kernel_test = train_test_split(X, y, c)\n",
        "\n",
        "          # train the data\n",
        "          classes = np.unique(y_train).astype(np.int32)\n",
        "          alpha, classes = kernel_perceptron_train(y_train, kernel_train, classes)\n",
        "\n",
        "          # calculate the rates of the errors (training data)\n",
        "          y_train_pred = kernel_perceptron_predict(alpha, classes, kernel_train)\n",
        "          train_error = compute_error(y_train, y_train_pred)\n",
        "\n",
        "          # calculate the rates of the errors (testing data)\n",
        "          y_test_pred = kernel_perceptron_predict(alpha, classes, kernel_test)\n",
        "          test_error = compute_error(y_test, y_test_pred)\n",
        "\n",
        "          compute_train_errors.append(train_error)\n",
        "          compute_run_test_errors.append(test_error)\n",
        "\n",
        "      train_errors.append(compute_train_errors)\n",
        "      test_errors.append(compute_run_test_errors)\n",
        "\n",
        "  #   for each degree, by averaging over the 20 runs, calculate mean and standard deviation for train and test errors\n",
        "  avg_train_errors = [(np.mean(errors), np.std(errors)) for errors in train_errors]\n",
        "  avg_test_errors = [(np.mean(errors), np.std(errors)) for errors in test_errors]\n",
        "\n",
        "\n",
        "  print(f\"{'(c): ':<20}{'Train Error':<30}{'Test Error':<30}\")\n",
        "  for c, (train, test) in zip(S, zip(avg_train_errors, avg_test_errors)):\n",
        "    train_mean, train_std = train\n",
        "    test_mean, test_std = test\n",
        "    print(f\"{c:.1e}{'':<10}{train_mean:.2f}% ± {train_std:.2f}%{'':<10}{test_mean:.2f}% ± {test_std:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "h22ist2qmzK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dec028c-eaaf-4db8-9118-e2ffddbd3ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(c):                Train Error                   Test Error                    \n",
            "7.8e-04          16.70% ± 3.12%          16.81% ± 3.17%\n",
            "1.6e-03          9.51% ± 2.08%          10.02% ± 1.80%\n",
            "3.4e-03          5.33% ± 0.10%          6.75% ± 0.45%\n",
            "7.0e-03          3.93% ± 0.11%          5.41% ± 0.55%\n",
            "1.4e-02          3.84% ± 0.11%          5.99% ± 0.40%\n",
            "3.0e-02          2.76% ± 0.09%          8.10% ± 0.76%\n",
            "6.2e-02          0.95% ± 0.05%          7.45% ± 0.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c)"
      ],
      "metadata": {
        "id": "pINuZS5JQo21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# gaussian kernel function: Kc(p, q) = e^(-c * ||p - q||^2)\n",
        "@jax.jit\n",
        "def gaussian_kernel(X1, X2, c):\n",
        "    squared_norm = jnp.sum((X1[:, None] - X2) ** 2, axis=2)\n",
        "    return jnp.exp(-c * squared_norm)\n",
        "\n",
        "def train_test_split(X, y):\n",
        "    # shuffle the data for and take 80% for training and 20% for testing\n",
        "    # split the data into training and test sets\n",
        "\n",
        "    ratio =0.8\n",
        "    shuffle = np.arange(len(X))\n",
        "    np.random.shuffle(shuffle)\n",
        "\n",
        "    shuffled_data_X = X[shuffle]\n",
        "    shuffled_data_y = y[shuffle]\n",
        "\n",
        "    split = int(len(X) * ratio)\n",
        "\n",
        "    X_train, X_test = shuffled_data_X[:split], shuffled_data_X[split:]\n",
        "    y_train, y_test = shuffled_data_y[:split], shuffled_data_y[split:]\n",
        "\n",
        "    # convert them into jax format for faster implementaion\n",
        "    y_train = jnp.array(y_train)\n",
        "    y_test = jnp.array(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# this function will train the data using One-vs-Rest method\n",
        "# a step by step explanation for OvR can be found on answer 2\n",
        "def kernel_perceptron_train(y_train, kernel_train, classes):\n",
        "    max_epochs = 50 # max number of epochs\n",
        "    n_samples = kernel_train.shape[0]\n",
        "    n_classes = len(classes)\n",
        "    # the wegiht is set to zero for each class\n",
        "    alpha = jnp.zeros((n_classes, n_samples), dtype=jnp.float32)\n",
        "    convergance_rate = 0.01\n",
        "\n",
        "    # this helper function aims to update the weight\n",
        "    def update_weights(state):\n",
        "        alpha, epoch,prev_updates = state\n",
        "        # for each sample, create binary labels for all classes\n",
        "        y_binary = jnp.where(y_train[:, None] == classes, 1, -1)\n",
        "        # compute the predictions for all samples and classes\n",
        "        predictions = jnp.sign(jnp.dot(alpha, kernel_train.T))\n",
        "        # identify where the predictions are incorrect and calculate the updates accordingly\n",
        "        # transposes y_binary to match the shape of predictions\n",
        "        updates = jnp.where(predictions != y_binary.T, y_binary.T, 0)\n",
        "        alpha = alpha + updates\n",
        "        avg_updates = jnp.mean(jnp.abs(updates)) # calculate the avg number of updates\n",
        "        return alpha, epoch + 1, avg_updates\n",
        "\n",
        "     # a helper function that returns true if epoch is less than max_epochs\n",
        "    def check_epoch(state):\n",
        "        _, epoch, avg_updates = state\n",
        "        return (epoch < max_epochs) & (avg_updates > convergance_rate)\n",
        "\n",
        "    # settings at the start : zero indicates to epoch=0, float('inf') as there's no previous update info\n",
        "    alpha, _ ,_= jax.lax.while_loop(check_epoch, update_weights, (alpha, 0, float('inf')))\n",
        "\n",
        "    return alpha, classes\n",
        "\n",
        "# prediction of (OvR) method\n",
        "@jax.jit\n",
        "def kernel_perceptron_predict(alpha, classes, kernel_matrix):\n",
        "    # calculate the weighted_sums for each class\n",
        "    weighted_sums = jnp.dot(alpha, kernel_matrix)\n",
        "    # take the test sample that has highest confidence score\n",
        "    predicted_indices = jnp.argmax(weighted_sums, axis=0)\n",
        "    return classes[predicted_indices]\n",
        "\n",
        "# compute error\n",
        "def compute_error(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    errors = jnp.sum(y_true != y_pred)\n",
        "    return (errors / n) * 100\n",
        "\n",
        "\n",
        "# precompute kernals to speed up the computaion of cross validation\n",
        "def precompute_kernels(X, S):\n",
        "    return {c: gaussian_kernel(X, X, c) for c in S}\n",
        "\n",
        "# cross validation function\n",
        "def cross_validate(X, y, S, precomputed_kernels):\n",
        "    folds=5\n",
        "    n_samples = len(X) # to get the number of samples in the dataset\n",
        "    fold_size = n_samples // folds # to obtain the size of each fold\n",
        "    arr_indices = np.arange(n_samples) # an array of ordered samples\n",
        "    np.random.shuffle(arr_indices)\n",
        "\n",
        "    best_c_errors = [] # list to store the the validation errors for each degree\n",
        "\n",
        "    for c in S:\n",
        "        fold_errors = [] # to store the errors for each fold\n",
        "        kernel = precomputed_kernels[c] # to obtain the precomputed kernals for the current degree\n",
        "\n",
        "        for fold in range(folds):\n",
        "            val_indices = arr_indices[fold * fold_size : (fold + 1) * fold_size] # select the indices for the validation set for the current fold\n",
        "            train_indices = np.setdiff1d(arr_indices, val_indices) # extract all the indices not in val_indices\n",
        "\n",
        "            # extract the training and validation kernals by slicing the precomputed kernal\n",
        "            kernel_train = kernel[train_indices][:, train_indices]\n",
        "            kernel_val = kernel[train_indices][:, val_indices]\n",
        "\n",
        "            y_train, y_val = y[train_indices], y[val_indices] # split the train and validation y\n",
        "            classes = np.unique(y_train).astype(np.int32)\n",
        "            alpha, _ = kernel_perceptron_train(y_train, kernel_train, classes) # train\n",
        "            y_val_pred = kernel_perceptron_predict(alpha, classes, kernel_val) # predict\n",
        "\n",
        "            val_error = compute_error(y_val, y_val_pred) # compute the error\n",
        "            fold_errors.append(val_error) # store the errors\n",
        "\n",
        "        mean_error = np.mean(fold_errors) # for the current polynomial degree, compute the mean of all folds\n",
        "        best_c_errors.append((c, mean_error))\n",
        "\n",
        "    best_c = min(best_c_errors, key=lambda x: x[1])[0] # retrieve the degree that has the lowest error\n",
        "    return best_c\n",
        "\n",
        "def main():\n",
        "  # read the data from zipcombo file and then convert the feature&labels\n",
        "  # the use of float32 here is to accelerate the execution time\n",
        "  data = []\n",
        "  with open('zipcombo.dat.txt', 'r') as f:\n",
        "      for line in f:\n",
        "              values = list(map(float, line.strip().split()))\n",
        "              data.append(values)\n",
        "      data = np.array(data, dtype=np.float32)\n",
        "  X = data[:, 1:].astype(jnp.float32)\n",
        "  y = data[:, 0].astype(jnp.float32)\n",
        "\n",
        "  S = [7.8e-4, 1.6e-3, 3.4e-3, 7.0e-3, 1.4e-2, 3.0e-2, 6.2e-2]\n",
        "  train_errors = []\n",
        "  test_errors = []\n",
        "  arr_best_c = []\n",
        "  confusion_matrices = []\n",
        "\n",
        "  # To speed up the excution time, we precompute full kernels for each degree\n",
        "  precomputed_kernels = precompute_kernels(X, S)\n",
        "\n",
        "  for _ in range(20):\n",
        "\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "      # perform cross validation with precomputed kernels to obtain the best degree\n",
        "      best_c = cross_validate(X_train, y_train, S, precomputed_kernels)\n",
        "      arr_best_c.append(best_c)\n",
        "\n",
        "      # now we retrain with the best degree\n",
        "      kernel_train = gaussian_kernel(X_train, X_train, best_c)\n",
        "      kernel_test = gaussian_kernel(X_train, X_test, best_c)\n",
        "      classes = np.unique(y_train).astype(np.int32)\n",
        "      alpha, classes = kernel_perceptron_train(y_train, kernel_train, classes)\n",
        "      # predict\n",
        "      y_train_pred = kernel_perceptron_predict(alpha, classes, kernel_train)\n",
        "      y_test_pred = kernel_perceptron_predict(alpha, classes, kernel_test)\n",
        "\n",
        "      # compute the errors of train and test\n",
        "      train_error = compute_error(y_train, y_train_pred)\n",
        "      test_error = compute_error(y_test, y_test_pred)\n",
        "\n",
        "      train_errors.append(train_error)\n",
        "      test_errors.append(test_error)\n",
        "\n",
        "\n",
        "# prin and plot the results\n",
        "  print(\"\\nCross-Validation Results:\")\n",
        "  for run in range(20):\n",
        "      print(f\"Run {run + 1}: Best c = {arr_best_c[run]:.1e}, Train Error = {train_errors[run]:.2f}%, Test Error = {test_errors[run]:.2f}%\")\n",
        "\n",
        "  mean_best_c = np.mean(arr_best_c)\n",
        "  std_best_c = np.std(arr_best_c)\n",
        "  mean_train_error = np.mean(train_errors)\n",
        "  std_train_error = np.std(train_errors)\n",
        "  mean_test_error = np.mean(test_errors)\n",
        "  std_test_error = np.std(test_errors)\n",
        "\n",
        "  print(\"\\n Across Runs :\")\n",
        "  print(f\"Best c* (mean ± std): {mean_best_c:.2f} ± {std_best_c:.2f}\")\n",
        "  print(f\"Train Error (mean ± std %): {mean_train_error:.2f}% ± {std_train_error:.2f}%\")\n",
        "  print(f\"Test Error (mean ± std %): {mean_test_error:.2f}% ± {std_test_error:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "e9q97EI5Qpkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079b3ca1-653c-4a6f-b54f-cab71ea6d6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Validation Results:\n",
            "Run 1: Best c = 6.2e-02, Train Error = 0.95%, Test Error = 7.37%\n",
            "Run 2: Best c = 3.0e-02, Train Error = 2.84%, Test Error = 9.14%\n",
            "Run 3: Best c = 3.4e-03, Train Error = 5.23%, Test Error = 7.15%\n",
            "Run 4: Best c = 6.2e-02, Train Error = 0.97%, Test Error = 6.29%\n",
            "Run 5: Best c = 7.0e-03, Train Error = 3.91%, Test Error = 5.27%\n",
            "Run 6: Best c = 7.8e-04, Train Error = 16.05%, Test Error = 17.63%\n",
            "Run 7: Best c = 7.0e-03, Train Error = 3.94%, Test Error = 5.59%\n",
            "Run 8: Best c = 3.0e-02, Train Error = 2.85%, Test Error = 8.71%\n",
            "Run 9: Best c = 3.0e-02, Train Error = 2.68%, Test Error = 8.60%\n",
            "Run 10: Best c = 3.0e-02, Train Error = 2.82%, Test Error = 8.33%\n",
            "Run 11: Best c = 7.0e-03, Train Error = 3.94%, Test Error = 5.05%\n",
            "Run 12: Best c = 6.2e-02, Train Error = 0.87%, Test Error = 7.63%\n",
            "Run 13: Best c = 3.0e-02, Train Error = 2.78%, Test Error = 7.20%\n",
            "Run 14: Best c = 3.0e-02, Train Error = 2.69%, Test Error = 8.55%\n",
            "Run 15: Best c = 3.4e-03, Train Error = 5.43%, Test Error = 6.34%\n",
            "Run 16: Best c = 3.0e-02, Train Error = 2.69%, Test Error = 8.55%\n",
            "Run 17: Best c = 1.4e-02, Train Error = 3.87%, Test Error = 6.40%\n",
            "Run 18: Best c = 6.2e-02, Train Error = 0.99%, Test Error = 5.97%\n",
            "Run 19: Best c = 1.6e-03, Train Error = 7.62%, Test Error = 7.96%\n",
            "Run 20: Best c = 6.2e-02, Train Error = 0.91%, Test Error = 7.90%\n",
            "\n",
            " Across Runs :\n",
            "Best c* (mean ± std): 0.03 ± 0.02\n",
            "Train Error (mean ± std %): 3.70% ± 3.30%\n",
            "Test Error (mean ± std %): 7.78% ± 2.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d)\n",
        "### **Basic results:**\n",
        "In **polynomial kernels**, the mean training error consistently decreases as the degree of the polynomial increases. This trend suggests that the model becomes better at fitting the training data with higher degrees, achieving its lowest training error between degrees 3 to 6. Interestingly, the standard deviation of the training error varies across the degrees, indicating some inconsistency in the model's ability to generalize across runs. The testing error, on the other hand, reduces up to degree 3, reaching its lowest point at 5.56%. Beyond degree 3, the testing error begins to increase slightly, signaling overfitting as the model becomes more tailored to the training set. In **Gaussian Kernels**, the training error behaves differenty. For small c values, the mean training error is higher, and both the mean and standard deviation decrease as c increases. The minimum training error is observed at c=6.2e−2, achieving an impressively low value of 0.95%. The testing error follows a similar pattern, with both the mean and standard deviation dropping as\n",
        "c increases. The lowest testing error is observed around c=7.0e−3 (5.41%). However, for c>7.0e−3, the testing error starts to increase, which clearly indicates overfitting.\n",
        "\n",
        "### **Cross-validation:**\n",
        "\n",
        "In **polynomial kernels**, the mean training error across runs is 4.11% ± 0.72%, showing the model's ability to generalize to the training data with a relatively small standard deviation, indicating consistent performance. The mean testing error is 6.29% ± 0.56%, which is slightly better than the Gaussian kernel in terms of consistency, suggesting that the polynomial kernel achieves a good balance between underfitting and overfitting. The best degree value varies between 2 and 5, with an average of 3.25 ± 0.99. In **Gaussian kernels** it achieves a mean training error of 3.70% ± 3.30%, which is slightly lower than the polynomial kernel, indicating better fitting to the training data. However, unlike the polynomial kernel, it exhibits a significantly higher standard deviation, reflecting less consistent results across runs. The mean test error for the Gaussian kernel is 7.78% ± 2.56%, which is higher than the polynomial kernel, indicating that it might overfit to the training data in some runs, leading to weaker generalization. The best c value is approximately  0.03 ± 0.02 and the low standard deviation suggests that the Gaussian kernel's performance remains consistent and reliable within this optimal range of the hyperparameter.\n",
        "\n",
        "### **overall**\n",
        "The Polynomial kernel demonstrates stronger generalization with a lower average test error (6.29% ± 0.56%) and more consistent results across runs, thanks to its smaller standard deviations. The optimal degree (3.25 ± 0.99) strikes a balance between model complexity and performance.\n",
        "\n",
        "On the other hand, the Gaussian kernel achieves a slightly lower average training error (3.70% ± 3.30%) but has a higher test error (7.78% ± 2.56%), indicating a tendency to overfit. Despite the optimal c value (0.03 ± 0.02) being relatively stable, the Gaussian kernel is more sensitive to hyperparameter choices and data variability. Overall, the Polynomial kernel proves to be a more reliable choice for this task."
      ],
      "metadata": {
        "id": "02uMeDXGcTjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 8."
      ],
      "metadata": {
        "id": "3I_AVdancW7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b)"
      ],
      "metadata": {
        "id": "-XyalMlacbhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@jax.jit\n",
        "def polynomial_kernel(X1, X2, d):\n",
        "    return jnp.power(jnp.dot(X1, X2.T), d)\n",
        "\n",
        "def train_test_split(X, y):\n",
        "    ratio=0.8\n",
        "    shuffle = np.arange(len(X))\n",
        "    np.random.shuffle(shuffle)\n",
        "\n",
        "    shuffled_data_X = X[shuffle]\n",
        "    shuffled_data_y = y[shuffle]\n",
        "\n",
        "    split = int(len(X) * ratio)\n",
        "    X_train, X_test = shuffled_data_X[:split], shuffled_data_X[split:]\n",
        "    y_train, y_test = shuffled_data_y[:split], shuffled_data_y[split:]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# kernal perceptron train based on OvO algorithm\n",
        "def ovo_kernel_perceptron_train(y_train, kernel_train, ci, cj, max_epochs=50, convergence_rate=0.01):\n",
        "    idx = jnp.where((y_train == ci) | (y_train == cj))[0] # select indices that belongs to ci or cj\n",
        "    X_pair = kernel_train[idx][:, idx]  # extract the subset of the kernel matrix corresponding to ci and cj\n",
        "    y_pair = jnp.where(y_train[idx] == ci, 1, -1) # assign +1 to class ci and -1 to class cj\n",
        "\n",
        "    alpha = jnp.zeros(len(idx), dtype=jnp.float32)  # initialize alpha\n",
        "\n",
        "    # update function for each epoch\n",
        "    def update_weights(state):\n",
        "        alpha, epoch, prev_avg_updates = state\n",
        "        predictions = jnp.sign(jnp.dot(alpha, X_pair)) #predict class labels using alpha and kernel matrix\n",
        "        updates = jnp.where(predictions != y_pair, y_pair, 0) #compare predictions with true labels\n",
        "        alpha = alpha + updates # update the weight\n",
        "        avg_updates = jnp.mean(jnp.abs(updates))\n",
        "        return alpha, epoch + 1, avg_updates\n",
        "\n",
        "    # check convergence condition\n",
        "    def check_epoch(state):\n",
        "        _, epoch, avg_updates = state\n",
        "        return (epoch < max_epochs) & (avg_updates > convergence_rate)\n",
        "\n",
        "    alpha, _, _ = jax.lax.while_loop(check_epoch, update_weights, (alpha, 0, float('inf')))\n",
        "\n",
        "    return alpha, idx\n",
        "\n",
        "# predict kernel perceptron based on OvO algorithm\n",
        "@jax.jit\n",
        "def ovo_kernel_perceptron_predict(X_test, kernel_test, classifiers, unique_classes):\n",
        "    votes = jnp.zeros((len(X_test), len(unique_classes)), dtype=jnp.int32) # initializes a zero matrix votes\n",
        "\n",
        "# a helper function to process a single binary classifier\n",
        "    def process_classifier(votes, alpha, idx, ci, cj):\n",
        "        binary_kernel  = kernel_test[:, idx]  # use only relevant kernel columns\n",
        "        predictions = jnp.sign(jnp.dot(binary_kernel , alpha))\n",
        "\n",
        "        # now update votes based on predictions\n",
        "        # if a prediction is +1, a vote is added for ci and, cj if the prediction is -1\n",
        "        votes = votes.at[:, ci].add(jnp.where(predictions > 0, 1, 0))\n",
        "        votes = votes.at[:, cj].add(jnp.where(predictions <= 0, 1, 0))\n",
        "\n",
        "        return votes\n",
        "\n",
        "    # loop through the classifiers and call the helper function to obtain the vote\n",
        "    for alpha, idx, ci, cj in classifiers:\n",
        "        votes = process_classifier(votes, alpha, idx, ci, cj)\n",
        "\n",
        "    # return the final prediction which is the class with the highest vote\n",
        "    return unique_classes[jnp.argmax(votes, axis=1)]\n",
        "\n",
        "def compute_error(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    errors = jnp.sum(y_true != y_pred)\n",
        "    return (errors / n) * 100\n",
        "\n",
        "def main():\n",
        "    # load dataset\n",
        "    data = []\n",
        "    with open('zipcombo.dat.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            values = list(map(float, line.strip().split()))\n",
        "            data.append(values)\n",
        "    data = np.array(data, dtype=np.float32)\n",
        "    X = data[:, 1:].astype(jnp.float32)\n",
        "    y = data[:, 0].astype(np.int32)\n",
        "\n",
        "    poly_degrees = range(1, 8)\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "\n",
        "    for d in poly_degrees:\n",
        "        compute_train_errors = []\n",
        "        compute_test_errors = []\n",
        "\n",
        "        for _ in range(20):\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "            kernel_train = polynomial_kernel(X_train, X_train, d)\n",
        "            kernel_test = polynomial_kernel(X_test, X_train, d)\n",
        "\n",
        "            unique_classes = np.unique(y_train)\n",
        "            classifiers = []\n",
        "\n",
        "            # to train one classifier per class\n",
        "            for i, ci in enumerate(unique_classes):\n",
        "                for cj in unique_classes[i + 1:]:\n",
        "                    alpha, idx = ovo_kernel_perceptron_train(y_train, kernel_train, ci, cj)\n",
        "                    classifiers.append((alpha, idx, ci, cj))\n",
        "\n",
        "            # predict on training and test sets\n",
        "            y_train_pred = ovo_kernel_perceptron_predict(X_train, kernel_train, classifiers, unique_classes)\n",
        "            y_test_pred = ovo_kernel_perceptron_predict(X_test, kernel_test, classifiers, unique_classes)\n",
        "\n",
        "            # compute error rates\n",
        "            train_error = compute_error(y_train, y_train_pred)\n",
        "            test_error = compute_error(y_test, y_test_pred)\n",
        "\n",
        "            compute_train_errors.append(train_error)\n",
        "            compute_test_errors.append(test_error)\n",
        "\n",
        "        train_errors.append(compute_train_errors)\n",
        "        test_errors.append(compute_test_errors)\n",
        "\n",
        "    # calculate mean and standard deviation for train and test errors\n",
        "    avg_train_errors = [(np.mean(errors), np.std(errors)) for errors in train_errors]\n",
        "    avg_test_errors = [(np.mean(errors), np.std(errors)) for errors in test_errors]\n",
        "\n",
        "    # print results\n",
        "    print(f\"{'Degree ':<10}{'Train Error':<30}{'Test Error':<30}\")\n",
        "    for d, (train, test) in zip(poly_degrees, zip(avg_train_errors, avg_test_errors)):\n",
        "        train_mean, train_std = train\n",
        "        test_mean, test_std = test\n",
        "        print(f\"{d:<10}{train_mean:.2f}% ± {train_std:.2f}%{'':<10}{test_mean:.2f}% ± {test_std:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMiyYJ8bcRdJ",
        "outputId": "729def7c-ade4-4d8f-c9cf-d9cfc5bab635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Degree    Train Error                   Test Error                    \n",
            "1         5.57% ± 0.64%          7.12% ± 0.95%\n",
            "2         4.31% ± 0.12%          6.54% ± 0.67%\n",
            "3         4.30% ± 0.08%          6.92% ± 0.46%\n",
            "4         4.26% ± 0.08%          7.40% ± 0.70%\n",
            "5         4.20% ± 0.09%          8.30% ± 0.78%\n",
            "6         4.13% ± 0.08%          8.84% ± 0.54%\n",
            "7         3.99% ± 0.09%          9.19% ± 0.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c)"
      ],
      "metadata": {
        "id": "y_RRSg8dcdLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# polynomial kernel Kd(p, q) = (p · q)^d\n",
        "@jax.jit\n",
        "def polynomial_kernel(X1, X2, d):\n",
        "    return jnp.power(jnp.dot(X1, X2.T), d)\n",
        "\n",
        "def train_test_split(X, y):\n",
        "    # shuffle the data for and take 80% for training and 20% for testing\n",
        "    # split the data into training and test sets\n",
        "\n",
        "    ratio =0.8\n",
        "    shuffle = np.arange(len(X))\n",
        "    np.random.shuffle(shuffle)\n",
        "\n",
        "    shuffled_data_X = X[shuffle]\n",
        "    shuffled_data_y = y[shuffle]\n",
        "\n",
        "    split = int(len(X) * ratio)\n",
        "\n",
        "    X_train, X_test = shuffled_data_X[:split], shuffled_data_X[split:]\n",
        "    y_train, y_test = shuffled_data_y[:split], shuffled_data_y[split:]\n",
        "\n",
        "    # convert them into jax format for faster implementaion\n",
        "    y_train = jnp.array(y_train)\n",
        "    y_test = jnp.array(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "# kernal perceptron train based on OvO algorithm\n",
        "def ovo_kernel_perceptron_train(y_train, kernel_train, ci, cj, max_epochs=50, convergence_rate=0.01):\n",
        "    idx = jnp.where((y_train == ci) | (y_train == cj))[0] # select indices that belongs to ci or cj\n",
        "    X_pair = kernel_train[idx][:, idx]  # extract the subset of the kernel matrix corresponding to ci and cj\n",
        "    y_pair = jnp.where(y_train[idx] == ci, 1, -1) # assign +1 to class ci and -1 to class cj\n",
        "\n",
        "    alpha = jnp.zeros(len(idx), dtype=jnp.float32)  # initialize alpha\n",
        "\n",
        "    # update function for each epoch\n",
        "    def update_weights(state):\n",
        "        alpha, epoch, prev_avg_updates = state\n",
        "        predictions = jnp.sign(jnp.dot(alpha, X_pair)) #predict class labels using alpha and kernel matrix\n",
        "        updates = jnp.where(predictions != y_pair, y_pair, 0) #compare predictions with true labels\n",
        "        alpha = alpha + updates # update the weight\n",
        "        avg_updates = jnp.mean(jnp.abs(updates))\n",
        "        return alpha, epoch + 1, avg_updates\n",
        "\n",
        "    # check convergence condition\n",
        "    def check_epoch(state):\n",
        "        _, epoch, avg_updates = state\n",
        "        return (epoch < max_epochs) & (avg_updates > convergence_rate)\n",
        "\n",
        "    alpha, _, _ = jax.lax.while_loop(check_epoch, update_weights, (alpha, 0, float('inf')))\n",
        "\n",
        "    return alpha, idx\n",
        "\n",
        "# predict kernel perceptron based on OvO algorithm\n",
        "@jax.jit\n",
        "def ovo_kernel_perceptron_predict(X_test, kernel_test, classifiers, unique_classes):\n",
        "    votes = jnp.zeros((len(X_test), len(unique_classes)), dtype=jnp.int32) # initializes a zero matrix votes\n",
        "\n",
        "# a helper function to process a single binary classifier\n",
        "    def process_classifier(votes, alpha, idx, ci, cj):\n",
        "        binary_kernel  = kernel_test[:, idx]  # use only relevant kernel columns\n",
        "        predictions = jnp.sign(jnp.dot(binary_kernel , alpha))\n",
        "\n",
        "        # now update votes based on predictions\n",
        "        # if a prediction is +1, a vote is added for ci and, cj if the prediction is -1\n",
        "        votes = votes.at[:, ci].add(jnp.where(predictions > 0, 1, 0))\n",
        "        votes = votes.at[:, cj].add(jnp.where(predictions <= 0, 1, 0))\n",
        "\n",
        "        return votes\n",
        "\n",
        "    # loop through the classifiers and call the helper function to obtain the vote\n",
        "    for alpha, idx, ci, cj in classifiers:\n",
        "        votes = process_classifier(votes, alpha, idx, ci, cj)\n",
        "\n",
        "    # return the final prediction which is the class with the highest vote\n",
        "    return unique_classes[jnp.argmax(votes, axis=1)]\n",
        "\n",
        "# compute error\n",
        "def compute_error(y_true, y_pred):\n",
        "    n = len(y_true)\n",
        "    errors = jnp.sum(y_true != y_pred)\n",
        "    return (errors / n) * 100\n",
        "\n",
        "# precompute kernels to speed up the computation\n",
        "def precompute_kernels(X, degrees):\n",
        "    return {d: polynomial_kernel(X, X, d) for d in degrees}\n",
        "\n",
        "# cross validation function\n",
        "def cross_validate(X, y, degrees, precomputed_kernels):\n",
        "    folds = 5\n",
        "    n_samples = len(X)\n",
        "    fold_size = n_samples // folds\n",
        "    arr_indices = np.arange(n_samples)\n",
        "    np.random.shuffle(arr_indices)\n",
        "\n",
        "    best_degree_errors = []\n",
        "\n",
        "    for d in degrees:\n",
        "        fold_errors = []\n",
        "        kernel = precomputed_kernels[d]\n",
        "\n",
        "        for fold in range(folds):\n",
        "            val_indices = arr_indices[fold * fold_size : (fold + 1) * fold_size]\n",
        "            train_indices = np.setdiff1d(arr_indices, val_indices)\n",
        "\n",
        "            kernel_train = kernel[train_indices][:, train_indices]\n",
        "            kernel_val = kernel[val_indices][:, train_indices]\n",
        "\n",
        "            y_train, y_val = y[train_indices], y[val_indices]\n",
        "            unique_classes = np.unique(y_train).astype(np.int32)\n",
        "\n",
        "            # train classifiers for OvO\n",
        "            classifiers = []\n",
        "            for i, ci in enumerate(unique_classes):\n",
        "                for cj in unique_classes[i + 1:]:\n",
        "                    alpha, idx = ovo_kernel_perceptron_train(y_train, kernel_train, ci, cj)\n",
        "                    classifiers.append((alpha, idx, ci, cj))\n",
        "\n",
        "            # predict validation set\n",
        "            y_val_pred = ovo_kernel_perceptron_predict(X[val_indices], kernel_val, classifiers, unique_classes)\n",
        "\n",
        "            # compute error\n",
        "            val_error = compute_error(y_val, y_val_pred)\n",
        "            fold_errors.append(val_error)\n",
        "\n",
        "        mean_error = np.mean(fold_errors) # for the current polynomial degree, compute the mean of all folds\n",
        "        best_degree_errors.append((d, mean_error))\n",
        "\n",
        "    best_d = min(best_degree_errors, key=lambda x: x[1])[0] # retrieve the degree that has the lowest error\n",
        "    return best_d\n",
        "\n",
        "def main():\n",
        "    # read the data from zipcombo file and then convert the feature&labels\n",
        "    # the use of float32 here is to accelerate the execution time\n",
        "    data = []\n",
        "    with open('zipcombo.dat.txt', 'r') as f:\n",
        "        for line in f:\n",
        "                values = list(map(float, line.strip().split()))\n",
        "                data.append(values)\n",
        "        data = np.array(data, dtype=np.float32)\n",
        "    X = data[:, 1:].astype(jnp.float32)\n",
        "    y = data[:, 0].astype(jnp.float32)\n",
        "\n",
        "    poly_degrees = range(1, 8)\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "    arr_best_degrees = []\n",
        "\n",
        "    # precompute kernels for all degrees\n",
        "    precomputed_kernels = precompute_kernels(X, poly_degrees)\n",
        "\n",
        "    for _ in range(20):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "        # perform cross-validation with precomputed kernels to obtain the best degree\n",
        "        best_degree = cross_validate(X_train, y_train, poly_degrees, precomputed_kernels)\n",
        "        arr_best_degrees.append(best_degree)\n",
        "\n",
        "        # retrain with the best degree\n",
        "        kernel_train = polynomial_kernel(X_train, X_train, best_degree)\n",
        "        kernel_test = polynomial_kernel(X_test, X_train, best_degree)\n",
        "\n",
        "        unique_classes = np.unique(y_train).astype(np.int32)\n",
        "        classifiers = []\n",
        "        # train using OvO algorithm\n",
        "        for i, ci in enumerate(unique_classes):\n",
        "            for cj in unique_classes[i + 1:]:\n",
        "                alpha, idx = ovo_kernel_perceptron_train(y_train, kernel_train, ci, cj)\n",
        "                classifiers.append((alpha, idx, ci, cj))\n",
        "\n",
        "        # predict on train and test sets\n",
        "        y_train_pred = ovo_kernel_perceptron_predict(X_train, kernel_train, classifiers, unique_classes)\n",
        "        y_test_pred = ovo_kernel_perceptron_predict(X_test, kernel_test, classifiers, unique_classes)\n",
        "\n",
        "        # compute errors\n",
        "        train_error = compute_error(y_train, y_train_pred)\n",
        "        test_error = compute_error(y_test, y_test_pred)\n",
        "\n",
        "        train_errors.append(train_error)\n",
        "        test_errors.append(test_error)\n",
        "\n",
        "    # print the output\n",
        "    print(\"\\nCross-Validation Results:\")\n",
        "    for run in range(20):\n",
        "        print(f\"Run {run + 1}: Best Degree = {arr_best_degrees[run]}, Train Error = {train_errors[run]:.2f}%, Test Error = {test_errors[run]:.2f}%\")\n",
        "\n",
        "    mean_best_degree = np.mean(arr_best_degrees)\n",
        "    std_best_degree = np.std(arr_best_degrees)\n",
        "    mean_train_error = np.mean(train_errors)\n",
        "    std_train_error = np.std(train_errors)\n",
        "    mean_test_error = np.mean(test_errors)\n",
        "    std_test_error = np.std(test_errors)\n",
        "\n",
        "    print(\"\\nAcross Runs:\")\n",
        "    print(f\"Best Degree (mean ± std): {mean_best_degree:.2f} ± {std_best_degree:.2f}\")\n",
        "    print(f\"Train Error (mean ± std %): {mean_train_error:.2f}% ± {std_train_error:.2f}%\")\n",
        "    print(f\"Test Error (mean ± std %): {mean_test_error:.2f}% ± {std_test_error:.2f}%\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RVZrJRRwaIs2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef6679b-da7b-4a1b-9334-c81bbd755f8c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Validation Results:\n",
            "Run 1: Best Degree = 1, Train Error = 5.16%, Test Error = 8.12%\n",
            "Run 2: Best Degree = 4, Train Error = 4.33%, Test Error = 6.18%\n",
            "Run 3: Best Degree = 5, Train Error = 4.06%, Test Error = 9.25%\n",
            "Run 4: Best Degree = 5, Train Error = 4.05%, Test Error = 6.94%\n",
            "Run 5: Best Degree = 4, Train Error = 4.21%, Test Error = 6.18%\n",
            "Run 6: Best Degree = 3, Train Error = 4.18%, Test Error = 6.77%\n",
            "Run 7: Best Degree = 1, Train Error = 5.66%, Test Error = 6.24%\n",
            "Run 8: Best Degree = 6, Train Error = 4.14%, Test Error = 9.30%\n",
            "Run 9: Best Degree = 4, Train Error = 4.37%, Test Error = 7.15%\n",
            "Run 10: Best Degree = 4, Train Error = 4.32%, Test Error = 6.51%\n",
            "Run 11: Best Degree = 4, Train Error = 4.49%, Test Error = 6.83%\n",
            "Run 12: Best Degree = 3, Train Error = 4.13%, Test Error = 6.99%\n",
            "Run 13: Best Degree = 2, Train Error = 4.19%, Test Error = 7.31%\n",
            "Run 14: Best Degree = 5, Train Error = 4.22%, Test Error = 7.26%\n",
            "Run 15: Best Degree = 3, Train Error = 4.32%, Test Error = 6.67%\n",
            "Run 16: Best Degree = 2, Train Error = 4.26%, Test Error = 6.08%\n",
            "Run 17: Best Degree = 3, Train Error = 4.42%, Test Error = 6.13%\n",
            "Run 18: Best Degree = 1, Train Error = 5.49%, Test Error = 6.72%\n",
            "Run 19: Best Degree = 7, Train Error = 4.02%, Test Error = 8.49%\n",
            "Run 20: Best Degree = 1, Train Error = 5.08%, Test Error = 7.53%\n",
            "\n",
            "Across Runs:\n",
            "Best Degree (mean ± std): 3.40 ± 1.69\n",
            "Train Error (mean ± std %): 4.45% ± 0.47%\n",
            "Test Error (mean ± std %): 7.13% ± 0.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) **Basic Results:** OvR achieves the best train and test errors at degree 3, with rates of 3.72% ± 0.09% and 5.56% ± 0.64%, respectively. It consistently exhibits lower test errors for higher degrees while maintaining smaller standard deviations compared to OvO. In contrast, OvO achieves its best train error at degree 7 (3.99% ± 0.09%) and best test error at degree 2 (6.54% ± 0.67%). However, unlike OvR, OvO shows higher test errors for higher polynomial degrees and generally exhibits larger standard deviations, indicating less stability.\n",
        "\n",
        "**Cross-Validation:** OvR achieves its best performance during cross-validation with a mean train error of 4.11% ± 0.72% and a mean test error of 6.29% ± 0.56%. Degrees 3 and 4 dominate as the optimal choices in most runs, further emphasizing the stability of the algorithm when selecting the best degree. On the other hand, OvO yields a mean train error of 4.45% ± 0.47% and a mean test error of 7.13% ± 0.95%, with higher variability in test errors, as evidenced by its larger standard deviation. The mean best degree appears to be more varied, indicating that it has less consistent results compared to OvR.\n",
        "\n",
        "**Overall:** OvR demonstrates superior stability and dependability, evidenced by smaller variability and lower mean errors. While OvO's paired classification strategy introduces narrower decision boundaries, it results in larger test errors and more variable outcomes. Consequently, OvR proves to be more stable and reliable than OvO for this particular dataset."
      ],
      "metadata": {
        "id": "b1Y8vpuiceg0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pI2t8PTFcf8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}