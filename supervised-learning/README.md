## Supervised Learning 

This repository contains two coursework projects covering core concepts in supervised learning, combining mathematical theory with full algorithm implementations from scratch.

Coursework 1 focuses on regression, kernel methods, and nearest neighbors. It includes linear regression with polynomial basis functions, analysis of underfitting and overfitting, experiments on noisy data, and evaluation of training versus test error behavior. The Boston Housing dataset is used to compare naive regression, single-attribute regression, full linear regression, and kernel ridge regression with Gaussian kernels. The coursework also includes an implementation of k-Nearest Neighbors and experiments illustrating the biasâ€“variance tradeoff. Theoretical components include kernel properties and a proof of the No-Free-Lunch theorem.

Coursework 2 centers on statistical learning theory and classification. It develops bounds using Rademacher complexity, analyzes the Bayes decision rule, and derives closed-form minimizers for squared, exponential, logistic, and hinge losses. Fisher consistency and comparison inequalities are proven for surrogate loss methods. The practical component implements multi-class kernel perceptron models using both One-vs-Rest and One-vs-One strategies, with polynomial and Gaussian kernels evaluated through cross-validation and error analysis.

Overall, the work demonstrates strong foundations in regression, classification, kernel methods, generalization theory, and empirical evaluation of learning algorithms.
